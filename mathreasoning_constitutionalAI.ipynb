{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2660a3ac",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Graded Lab: Constitutional AI for Mathematical Reasoning\n",
    "\n",
    "Welcome to this assignment!\n",
    "\n",
    "Carefully read each Markdown (text) cell, which include instructions and hints. Start by reading the background behind your upcoming tasks.\n",
    "\n",
    "When you are done, submit your solution by saving it, then clicking on the submit button at the top right side of the page.\n",
    "\n",
    "## In order for your submission to be graded correctly, you **MUST**:\n",
    "* **Use the provided variable names**, otherwise the autograder will not be able to locate the variable for grading. \n",
    "\n",
    "* **Replace any instances of `None` with your own code.** \n",
    "\n",
    "* **Only modify the cells that start with the comment `# GRADED CELL`**.  \n",
    "\n",
    "* **Use the provided cells for your solution.** You can add new cells to experiment, but these will be omitted when grading. \n",
    "\n",
    "To submit your solution, save it, then click on the blue submit button at the top of the page.\n",
    "\n",
    "<div style=\"background-color: #FAD888; padding: 10px; border-radius: 3px; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1); width:95%\n",
    "\">\n",
    "<strong>Important notes</strong>:\n",
    "\n",
    "- Code blocks with None will not run properly. If you run them before completing the exercise, you will likely get an error. \n",
    "\n",
    "- The notebooks work best in Chrome browser. If you are having problems, please switch to Chrome.\n",
    "\n",
    "- Make sure you always save before submitting.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7363bcca-1aaf-4c3f-ae80-f1540bcbfe17",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab, you'll build a pipeline that generates multiple solution approaches to math problems and then evaluates them using constitutional principles. This assignment will deepen your understanding of template engineering, solution diversity, and constitution alignment techniques.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will implement a comprehensive constitutional AI system that generates diverse mathematical solutions and evaluates them against quality principles. Through hands-on exercises, you'll learn to engineer effective templates, create solution diversity, and apply constitutional principles to rank mathematical reasoning approaches. You will:\n",
    "\n",
    "* **Create Solution Templates:** Design prompt templates for generating Chain-of-Thought and verification-based solutions that encourage clear mathematical reasoning and answer checking.\n",
    "* **Generate Alternative Method Solutions:** Build dynamic prompts that analyze existing solutions and generate genuinely different mathematical approaches using varied problem-solving strategies.\n",
    "* **Implement Accuracy Assessment:** Create a robust numerical answer extraction system that compares model solutions against ground truth values with appropriate tolerance handling.\n",
    "* **Evaluate Solution Completeness:** Develop comprehensive scoring systems that assess intermediate steps, calculations, and explanatory reasoning in mathematical solutions using pattern recognition.\n",
    "* **Assess Verification Quality:** Use LLM-as-judge techniques to evaluate whether solutions include proper answer verification and sanity checking procedures.\n",
    "* **Measure Solution Novelty:** Implement comparative analysis to determine whether alternative solutions use genuinely different mathematical approaches rather than mere rewordings of existing methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f883d4eb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Setup](#setup)\n",
    "* [Template Engineering](#templateengineering) - Exercise 1\n",
    "* [Prompt Formatting](#promptformatting)\n",
    "* [Generate Chain-of-Thought Solutions](#generatecot)\n",
    "* [Generate Alternative Method Solutions](#alternative) - Exercise 2\n",
    "* [Constitutional Principles Foundation](#constitutional) - Exercise 3\n",
    "* [Assessments](#assessments) - Exercise 4, 5, 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dda85cc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Setup <a id=\"setup\"></a>\n",
    "\n",
    "As usual, start by importing the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fedfe3ee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import utility functions\n",
    "from utils import setup_model_and_tokenizer, load_gsm8k_dataset, save_results, display_evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfe0a014",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Problem:\n",
      "Question: Maddox and Theo both bought 3 Polaroid Cameras, each sold at $20 per camera from Amazon, and decided to sell them on eBay. Maddox sold his cameras at $28 each, while Theo sold his cameras at $23 each. How much more profit did Maddox get from the sale of his cameras than Theo?\n",
      "Ground Truth Answer: 15.0\n",
      "\n",
      "Dataset contains 50 problems\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and examine sample\n",
    "dataset = load_gsm8k_dataset()\n",
    "sample = dataset[0]\n",
    "print(\"\\nSample Problem:\")\n",
    "print(f\"Question: {sample['question']}\")\n",
    "print(f\"Ground Truth Answer: {sample['ground_truth_answer']}\")\n",
    "print(f\"\\nDataset contains {len(dataset)} problems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262977e8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "You are using **Meta-Llama-3.2-8B-Instruct**, a powerful language model fine-tuned for following instructions and generating high-quality text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dcd0ef1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: AMD Instinct MI300X VF\n",
      "GPU Memory: 191.7 GB\n",
      "\n",
      "Loading /app/models/llama-3.2-8b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f2583916ee4b878b0390286fe2d512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully in 7.8 seconds!\n"
     ]
    }
   ],
   "source": [
    "# Load model using utility function\n",
    "model, tokenizer = setup_model_and_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b945f14",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Template Engineering <a id=\"templateengineering\"></a>\n",
    "\n",
    "Template Engineering is the foundation of generating diverse, high-quality mathematical solutions. This part focuses on creating three distinct solution approaches and implementing efficient batch processing for scalable generation. The Templates class manages different solution approaches. You start with two base templates, the third approach (alternative) will be created dynamically later using the CoT solutions as context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f18746f-c1d9-46fe-9894-cd0517952afc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Exercise 1: Create Solution Templates\n",
    "\n",
    "Your task here is to write a template to generate \n",
    "1. A Chain-Of-Thought solution  \n",
    "2. A solution that includes a verification step \n",
    "\n",
    "**Note**: You only define two templates here because the alternative approach needs to see the CoT solution first to ensure it uses a genuinely different method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76a3df6a",
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CELL: exercise 1\n",
    "\n",
    "class Templates:\n",
    "    \"\"\"Manages different solution templates and batched generation\"\"\"\n",
    "    \n",
    "    def __init__(self, model=None, tokenizer=None):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        # Complete the COT and verification templates to encourage different mathematical approaches\n",
    "        # COT: Solve problem step by step, Verification: Verify that the answer makes sense\n",
    "        \n",
    "        # IMPORTANT: Include {problem} in your template where the math problem should be inserted\n",
    "        # This placeholder will be replaced with the actual problem text using .format()\n",
    "        self.templates = { \n",
    "            \"cot\": \"\"\"Solve the given problem step by step showing clear reasoning\\n problem: {problem}\"\"\",\n",
    "            \"verification\": \"\"\"Verify that the answer makes sense\\n...problem: {problem}\"\"\"\n",
    "        } \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "# Initialize templates\n",
    "templates = Templates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed83d91",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jp-MarkdownHeadingCollapsed": false
   },
   "source": [
    "## Prompt Formatting <a id=\"promptformatting\"></a>\n",
    "\n",
    "LLaMA models require specific chat formatting. This function ensures prompts are properly structured for the model with a reliable fallback method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e1a4d4f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def format_prompts_for_batch(tokenizer, prompts):\n",
    "    \"\"\"Format multiple prompts for batch processing\"\"\"\n",
    "    formatted_prompts = []\n",
    "    for p in prompts:\n",
    "        try:\n",
    "            formatted = tokenizer.apply_chat_template(\n",
    "                [{\"role\": \"user\", \"content\": p}], \n",
    "                tokenize=False, \n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "        except:\n",
    "            formatted = f\"<s>[INST] {p} [/INST]\"\n",
    "        formatted_prompts.append(formatted)\n",
    "    return formatted_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef99014f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "To process multiple prompts efficiently, you tokenize them in batches. This function converts the formatted text prompts into token IDs, applies padding for uniform length, and moves tensors to the model's device (GPU/CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1daa9b60",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def tokenize_batch(tokenizer, formatted_prompts, model):\n",
    "    \"\"\"Tokenize batch of prompts and move to device\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        formatted_prompts, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    \n",
    "    return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ab8d36",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "This function performs the actual text generation using the tokenized inputs. It runs inference on the model with specified generation parameters like temperature and maximum token length, while using memory-efficient processing with torch.no_grad()\n",
    "\n",
    "**Key Parameters**:\n",
    "- `max_tokens=600`: Maximum length for generated solutions\n",
    "- `temperature=0.3`: Controls randomness (lower = more consistent)\n",
    "- Proper memory management with `torch.no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bb358b7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def generate_batch(model, tokenizer, input_ids, attention_mask, max_tokens=600, temperature=0.3):\n",
    "    \"\"\"Generate responses for tokenized batch\"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45f727e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "After generation, you extract only the newly generated tokens by slicing off the original input length, then decode these tokens back to readable text. This ensures you only return the model's response, not the input prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7725be37",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def decode_batch_outputs(tokenizer, outputs, input_ids):\n",
    "    \"\"\"Decode generated outputs to text\"\"\"\n",
    "    batch_results = []\n",
    "    for j, output in enumerate(outputs):\n",
    "        actual_input_len = input_ids[j].shape[0]\n",
    "        gen_tokens = output[actual_input_len:]\n",
    "        text = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "        batch_results.append(text)\n",
    "    return batch_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8a6587",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jp-MarkdownHeadingCollapsed": false
   },
   "source": [
    "## Generate Chain-of-Thought Solutions  <a id=\"generatecot\"></a>\n",
    "\n",
    "Step 1 of the dataset generation pipeline: Create detailed, step-by-step reasoning solutions for one problem to test that your prompt works well. These solutions will serve as the baseline and will be used to inform the alternative solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8253a609-65f0-4a95-abdd-709d24966b78",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Check the generated COT solution for your prompt. Edit section 2.1, if you are not satisfied with the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7bbb63b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating CoT responses for 1 problem...\n",
      "\n",
      "Testing CoT template:\n",
      "Problem: Maddox and Theo both bought 3 Polaroid Cameras, each sold at $20 per camera from Amazon, and decided to sell them on eBay. Maddox sold his cameras at $28 each, while Theo sold his cameras at $23 each. How much more profit did Maddox get from the sale of his cameras than Theo?\n",
      "Ground Truth: 15.0\n",
      "\n",
      "CoT Solution:\n",
      "To find out how much more profit Maddox got from the sale of his cameras than Theo, we need to calculate the profit made by each of them and then find the difference.\n",
      "\n",
      "**Step 1: Calculate the cost price of the cameras**\n",
      "Maddox and Theo bought 3 cameras each at $20 per camera.\n",
      "Cost price of 3 cameras = 3 x $20 = $60\n",
      "\n",
      "**Step 2: Calculate the profit made by Maddox**\n",
      "Maddox sold his cameras at $28 each.\n",
      "Selling price of 3 cameras = 3 x $28 = $84\n",
      "Profit made by Maddox = Selling price - Cost price\n",
      "= $84 - $60\n",
      "= $24\n",
      "\n",
      "**Step 3: Calculate the profit made by Theo**\n",
      "Theo sold his cameras at $23 each.\n",
      "Selling price of 3 cameras = 3 x $23 = $69\n",
      "Profit made by Theo = Selling price - Cost price\n",
      "= $69 - $60\n",
      "= $9\n",
      "\n",
      "**Step 4: Calculate the difference in profit**\n",
      "To find out how much more profit Maddox got than Theo, we subtract Theo's profit from Maddox's profit.\n",
      "Difference in profit = Profit made by Maddox - Profit made by Theo\n",
      "= $24 - $9\n",
      "= $15\n",
      "\n",
      "Therefore, Maddox got $15 more profit from the sale of his cameras than Theo.\n"
     ]
    }
   ],
   "source": [
    "def generate_cot_solutions(templates, dataset, num_problems=5):\n",
    "    \"\"\"Generate Chain-of-Thought solutions\"\"\"\n",
    "    print(f\"Generating CoT responses for {num_problems} problem{'s' if num_problems > 1 else ''}...\")\n",
    "    \n",
    "    problems = dataset[:num_problems]\n",
    "    cot_prompts = [templates.templates[\"cot\"].format(problem=p[\"question\"]) for p in problems]\n",
    "    \n",
    "    # Process in batches\n",
    "    cot_responses = []\n",
    "    batch_size = 2\n",
    "    \n",
    "    for i in range(0, len(cot_prompts), batch_size):\n",
    "        batch_prompts = cot_prompts[i:i+batch_size]\n",
    "        formatted = format_prompts_for_batch(tokenizer, batch_prompts)\n",
    "        input_ids, attention_mask = tokenize_batch(tokenizer, formatted, model)\n",
    "        outputs = generate_batch(model, tokenizer, input_ids, attention_mask)\n",
    "        batch_results = decode_batch_outputs(tokenizer, outputs, input_ids)\n",
    "        cot_responses.extend(batch_results)\n",
    "    \n",
    "    return cot_prompts, cot_responses\n",
    "\n",
    "# Test CoT template on a single problem\n",
    "cot_prompts, cot_responses = generate_cot_solutions(templates, dataset, 1)\n",
    "print(f\"\\nTesting CoT template:\")\n",
    "print(f\"Problem: {dataset[0]['question']}\")\n",
    "print(f\"Ground Truth: {dataset[0]['ground_truth_answer']}\")\n",
    "print(f\"\\nCoT Solution:\\n{cot_responses[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc44f7e1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Check the generated verification solution for your prompt. Edit section 2.1, if you are not satisfied with the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c68a810",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating verification responses for 1 problem...\n",
      "\n",
      "Testing Verification template:\n",
      "Verification Solution:\n",
      "To find the profit difference, we need to calculate the profit made by each person.\n",
      "\n",
      "Maddox bought 3 cameras at $20 each, so his total cost is 3 x $20 = $60. \n",
      "He sold each camera for $28, so his total revenue is 3 x $28 = $84. \n",
      "His profit is $84 - $60 = $24.\n",
      "\n",
      "Theo bought 3 cameras at $20 each, so his total cost is 3 x $20 = $60. \n",
      "He sold each camera for $23, so his total revenue is 3 x $23 = $69. \n",
      "His profit is $69 - $60 = $9.\n",
      "\n",
      "The difference in profit between Maddox and Theo is $24 - $9 = $15.\n",
      "\n",
      "Therefore, Maddox got $15 more profit from the sale of his cameras than Theo.\n"
     ]
    }
   ],
   "source": [
    "def generate_verification_solutions(templates, dataset, num_problems=5):\n",
    "    \"\"\"Generate verification solutions\"\"\"\n",
    "    print(f\"Generating verification responses for {num_problems} problem{'s' if num_problems > 1 else ''}...\")\n",
    "    \n",
    "    problems = dataset[:num_problems]\n",
    "    verification_prompts = [templates.templates[\"verification\"].format(problem=p[\"question\"]) for p in problems]\n",
    "    \n",
    "    # Process in batches\n",
    "    verification_responses = []\n",
    "    batch_size = 2\n",
    "    \n",
    "    for i in range(0, len(verification_prompts), batch_size):\n",
    "        batch_prompts = verification_prompts[i:i+batch_size]\n",
    "        formatted = format_prompts_for_batch(tokenizer, batch_prompts)\n",
    "        input_ids, attention_mask = tokenize_batch(tokenizer, formatted, model)\n",
    "        outputs = generate_batch(model, tokenizer, input_ids, attention_mask)\n",
    "        batch_results = decode_batch_outputs(tokenizer, outputs, input_ids)\n",
    "        verification_responses.extend(batch_results)\n",
    "    \n",
    "    return verification_prompts, verification_responses\n",
    "\n",
    "# Test verification template on the same problem\n",
    "verification_prompts, verification_responses = generate_verification_solutions(templates, dataset, 1)\n",
    "print(f\"\\nTesting Verification template:\")\n",
    "print(f\"Verification Solution:\\n{verification_responses[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4397d3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jp-MarkdownHeadingCollapsed": false
   },
   "source": [
    "## Generate Alternative Method Solutions <a id=\"alternative\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e338c07-0cbd-4960-856f-3c8aa37cd8cf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Exercise 2: Generate Alternative Method Solutions\n",
    "\n",
    "Create solutions using different mathematical approaches. Unlike the first two templates, these prompts are created dynamically by showing the model the CoT solution and asking for a different approach.\n",
    "\n",
    "Complete the alternative prompt template to encourage truly different mathematical approaches. The template should encourage different mathematical approaches like working backwards, using different operations, or visual reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6d4eff4",
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating alternative responses for 1 problem...\n",
      "\n",
      "Testing Alternative template:\n",
      "Alternative Solution:\n",
      "To solve this problem using an alternative approach, let's use a different method: finding the profit percentage and then comparing it.\n",
      "\n",
      "**Step 1: Calculate the profit percentage for each seller**\n",
      "Maddox sold his cameras at $28 each, which is 140% of the cost price ($20 x 1.4 = $28).\n",
      "Theo sold his cameras at $23 each, which is 115.5% of the cost price ($20 x 1.155 = $23).\n",
      "\n",
      "**Step 2: Calculate the profit percentage difference**\n",
      "The profit percentage difference between Maddox and Theo is 140% - 115.5% = 24.5%.\n",
      "\n",
      "**Step 3: Calculate the profit difference**\n",
      "Since the cost price is the same for both ($60), the profit difference is directly proportional to the profit percentage difference.\n",
      "To find the actual profit difference, multiply the cost price by the profit percentage difference (as a decimal).\n",
      "Profit difference = $60 x 0.245 = $14.7\n",
      "\n",
      "However, we can also express the profit difference as a percentage of the cost price, which is the same as the profit percentage difference (24.5%).\n",
      "To find the actual profit difference, multiply the cost price by the profit percentage difference (as a decimal).\n",
      "Profit difference = $60 x 0.245 = $14.7\n",
      "\n",
      "Alternatively, we can express the profit difference as a percentage of the selling price. Since the selling price is the same for both, we can use the profit percentage difference to find the profit difference.\n",
      "Profit difference = (Profit percentage difference / 100) x Selling price\n",
      "= (24.5 / 100) x $84\n",
      "= $20.62\n",
      "\n",
      "However, we can simplify this calculation by using the fact that the profit percentage difference is the same as the profit difference divided by the cost price.\n",
      "Profit difference = Profit percentage difference x Cost price\n",
      "= 24.5% x $60\n",
      "= 0.245 x $60\n",
      "= $14.7\n",
      "\n",
      "We can also express the profit difference as a percentage of the profit made by Maddox.\n",
      "Profit difference = (Profit percentage difference / 100) x Profit made by Maddox\n",
      "= (24.5 / 100) x $24\n",
      "= $5.88\n",
      "\n",
      "However, we can simplify this calculation by using the fact that the profit percentage difference is the same as the profit difference divided by the cost price.\n",
      "Profit difference = Profit percentage difference x Cost price\n",
      "= 24.5% x $60\n",
      "= 0.245 x $60\n",
      "= $14.7\n",
      "\n",
      "We can also express the profit difference as a percentage of the profit made by Theo.\n",
      "Profit difference = (Profit percentage difference / 100) x Profit made by Theo\n",
      "= (24.5 / 100) x $9\n",
      "= $2.215\n",
      "\n",
      "However, we can simplify this calculation by using the fact that the profit percentage\n"
     ]
    }
   ],
   "source": [
    "# GRADED CELL: exercise 2\n",
    "\n",
    "def generate_alternative_solutions(dataset, cot_responses, num_problems=5):\n",
    "    \"\"\"Generate alternative approach solutions\"\"\"\n",
    "    print(f\"Generating alternative responses for {num_problems} problem{'s' if num_problems > 1 else ''}...\")\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Create a template that shows the student an existing solution and asks for a different approach\n",
    "    # Remember: Use {problem} and {cot_response} as placeholders that will be filled in later\n",
    "    alt_prompt_template = \"\"\"You are an expert in solving mathematical problems. \n",
    "    Solve the given problem step-by-step using an alternative mathematical approach like working backwards or using different operations.\n",
    "    The solution must be different from the given chain-of-thought response.\n",
    "\n",
    "    Problem: {problem}\n",
    "\n",
    "    Response: {cot_response}\n",
    "    \n",
    "    \"\"\"\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    problems = dataset[:num_problems]\n",
    "    alt_prompts = []\n",
    "    \n",
    "    for i, problem_data in enumerate(problems):\n",
    "        alt_prompt = alt_prompt_template.format(\n",
    "            problem=problem_data[\"question\"], \n",
    "            cot_response=cot_responses[i]\n",
    "        )\n",
    "        alt_prompts.append(alt_prompt)\n",
    "    \n",
    "    # Process in batches\n",
    "    alt_responses = []\n",
    "    batch_size = 2\n",
    "    \n",
    "    for i in range(0, len(alt_prompts), batch_size):\n",
    "        batch_prompts = alt_prompts[i:i+batch_size]\n",
    "        formatted = format_prompts_for_batch(tokenizer, batch_prompts)\n",
    "        input_ids, attention_mask = tokenize_batch(tokenizer, formatted, model)\n",
    "        outputs = generate_batch(model, tokenizer, input_ids, attention_mask)\n",
    "        batch_results = decode_batch_outputs(tokenizer, outputs, input_ids)\n",
    "        alt_responses.extend(batch_results)\n",
    "    \n",
    "    return alt_prompts, alt_responses\n",
    "\n",
    "# Test alternative template on the same problem\n",
    "alt_prompts, alt_responses = generate_alternative_solutions(dataset, cot_responses, 1)\n",
    "print(f\"\\nTesting Alternative template:\")\n",
    "print(f\"Alternative Solution:\\n{alt_responses[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714dbbb6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "This function combines all three solution types into the structured dataset format required for constitutional evaluation.\n",
    "\n",
    "**Output Format**: Each result contains the original problem, ground truth answer, and all three solutions with their templates, prompts, and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cefe1737",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def assemble_solution_dataset(dataset, cot_data, verification_data, alternative_data, num_problems):\n",
    "    \"\"\"Combine all solution types into structured dataset\"\"\"\n",
    "    cot_prompts, cot_responses = cot_data\n",
    "    verification_prompts, verification_responses = verification_data\n",
    "    alt_prompts, alt_responses = alternative_data\n",
    "    \n",
    "    results = []\n",
    "    problems = dataset[:num_problems]\n",
    "    \n",
    "    for i, problem_data in enumerate(problems):\n",
    "        problem = problem_data[\"question\"]\n",
    "        ground_truth = str(problem_data[\"ground_truth_answer\"])\n",
    "        \n",
    "        solutions = [\n",
    "            {\"template\": \"cot\", \"prompt\": cot_prompts[i], \"response\": cot_responses[i]},\n",
    "            {\"template\": \"verification\", \"prompt\": verification_prompts[i], \"response\": verification_responses[i]},\n",
    "            {\"template\": \"alternative\", \"prompt\": alt_prompts[i], \"response\": alt_responses[i]},\n",
    "        ]\n",
    "        \n",
    "        result = {\n",
    "            \"problem_id\": i,\n",
    "            \"question\": problem,\n",
    "            \"ground_truth_answer\": ground_truth,\n",
    "            \"solutions\": solutions\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14f924e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now that you've tested all templates and verified their output quality, generate solutions for the entire dataset. Notice that `NUM_PROBLEMS` is set to 50. Try starting with 5-10 problems for initial testing, then scale up to 50 for full analysis.\n",
    "\n",
    "It should take no more than 12 minutes to run this cell on all 50 problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eb2506e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating complete dataset for 50 problems...\n",
      "\n",
      "==================================================\n",
      "GENERATING FULL DATASET\n",
      "==================================================\n",
      "Generating CoT responses for 50 problems...\n",
      "Generating verification responses for 50 problems...\n",
      "Generating alternative responses for 50 problems...\n",
      "Results saved to generated_solutions.json\n",
      "\n",
      "Generation complete! Dataset with 50 problems saved to generated_solutions.json\n",
      "\n",
      "============================================================\n",
      "Problem 1: Maddox and Theo both bought 3 Polaroid Cameras, each sold at $20 per camera from Amazon, and decided to sell them on eBay. Maddox sold his cameras at $28 each, while Theo sold his cameras at $23 each. How much more profit did Maddox get from the sale of his cameras than Theo?\n",
      "Ground Truth: 15.0\n",
      "\n",
      "--- COT Solution ---\n",
      "To find out how much more profit Maddox got from the sale of his cameras than Theo, we need to calculate the profit made by each of them and then find the difference.\n",
      "\n",
      "**Step 1: Calculate the cost pri...\n",
      "\n",
      "--- VERIFICATION Solution ---\n",
      "To find out how much profit each person made, we need to calculate the total revenue and subtract the total cost.\n",
      "\n",
      "Maddox bought 3 cameras at $20 each, so his total cost is:\n",
      "3 * $20 = $60\n",
      "\n",
      "Maddox sold...\n",
      "\n",
      "--- ALTERNATIVE Solution ---\n",
      "To solve this problem using an alternative approach, let's use the concept of percentage profit and the difference in selling prices.\n",
      "\n",
      "**Step 1: Find the difference in selling prices**\n",
      "Difference in S...\n",
      "\n",
      "============================================================\n",
      "Problem 2: A baker is making bread according to a recipe that requires him to use 3 eggs for every 2 cups of flour.  If the baker wants to use up the 6 cups of flour he has remaining in his pantry, how many eggs will he need to use?\n",
      "Ground Truth: 9.0\n",
      "\n",
      "--- COT Solution ---\n",
      "To solve this problem, we need to find out how many eggs the baker needs for 6 cups of flour.\n",
      "\n",
      "Step 1: Determine the ratio of eggs to flour in the recipe.\n",
      "The recipe requires 3 eggs for every 2 cups o...\n",
      "\n",
      "--- VERIFICATION Solution ---\n",
      "To find out how many eggs the baker needs, we need to determine the ratio of eggs to flour in the recipe. \n",
      "\n",
      "The given ratio is 3 eggs for every 2 cups of flour. \n",
      "\n",
      "To find out how many eggs the baker n...\n",
      "\n",
      "--- ALTERNATIVE Solution ---\n",
      "To solve this problem using an alternative approach, let's use the concept of proportions. We can set up a proportion to find the number of eggs needed for 6 cups of flour.\n",
      "\n",
      "Let x be the number of egg...\n",
      "\n",
      "============================================================\n",
      "Solution generation complete! Ready for constitutional evaluation.\n"
     ]
    }
   ],
   "source": [
    "NUM_PROBLEMS = 50\n",
    "\n",
    "print(f\"Generating complete dataset for {NUM_PROBLEMS} problems...\")\n",
    "\n",
    "# Generate all solution types for the full dataset\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GENERATING FULL DATASET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Step 1: Generate CoT solutions for full dataset\n",
    "cot_prompts_full, cot_responses_full = generate_cot_solutions(templates, dataset, NUM_PROBLEMS)\n",
    "\n",
    "# Step 2: Generate verification solutions for full dataset\n",
    "verification_prompts_full, verification_responses_full = generate_verification_solutions(templates, dataset, NUM_PROBLEMS)\n",
    "\n",
    "# Step 3: Generate alternative solutions for full dataset\n",
    "alt_prompts_full, alt_responses_full = generate_alternative_solutions(dataset, cot_responses_full, NUM_PROBLEMS)\n",
    "\n",
    "# Step 4: Assemble complete dataset\n",
    "solution_results = assemble_solution_dataset(\n",
    "    dataset, \n",
    "    (cot_prompts_full, cot_responses_full), \n",
    "    (verification_prompts_full, verification_responses_full), \n",
    "    (alt_prompts_full, alt_responses_full), \n",
    "    NUM_PROBLEMS\n",
    ")\n",
    "\n",
    "# Save results\n",
    "save_results(solution_results, \"generated_solutions.json\")\n",
    "print(f\"\\nGeneration complete! Dataset with {len(solution_results)} problems saved to generated_solutions.json\")\n",
    "\n",
    "# Display sample results for review\n",
    "for i, result in enumerate(solution_results[:2]):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Problem {i+1}: {result['question']}\")\n",
    "    print(f\"Ground Truth: {result['ground_truth_answer']}\")\n",
    "    \n",
    "    for solution in result['solutions']:\n",
    "        print(f\"\\n--- {solution['template'].upper()} Solution ---\")\n",
    "        response_preview = solution['response'][:200] + \"...\" if len(solution['response']) > 200 else solution['response']\n",
    "        print(response_preview)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Solution generation complete! Ready for constitutional evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dcd751",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Constitutional Principles Foundation <a id=\"constitutional\"></a>\n",
    "\n",
    "The Constitutional Evaluator applies four key principles to assess solution quality. Each principle targets a specific aspect of mathematical reasoning to encourage.\n",
    "\n",
    "**The Four Principles**:\n",
    "- **Accuracy**: All three solutions must be correct\n",
    "- **Completeness**: All three solutions must show all intermediate calculation steps\n",
    "- **Verification**: Solution 2 should include verification or sanity checking\n",
    "- **Novelty**: Solution 3 must use a different approach from solution 1 to solve the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ebfdefd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constitutional Principles:\n",
      "  - accuracy: All three solutions must be correct\n",
      "  - completeness: All three solutions must show all intermediate calculation steps\n",
      "  - verification: Solution 2 should include verification or sanity checking\n",
      "  - novelty: Solution 3 must use a different approach from solution 1 to solve the problem\n",
      "Using LLM as judge for verification and novelty evaluation\n"
     ]
    }
   ],
   "source": [
    "class ConstitutionalEvaluator:\n",
    "    \"\"\"Constitutional AI system for evaluating mathematical reasoning\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.principles = {\n",
    "            \"accuracy\": \"All three solutions must be correct\",\n",
    "            \"completeness\": \"All three solutions must show all intermediate calculation steps\", \n",
    "            \"verification\": \"Solution 2 should include verification or sanity checking\",\n",
    "            \"novelty\": \"Solution 3 must use a different approach from solution 1 to solve the problem\"\n",
    "        }\n",
    "        print(\"Constitutional Principles:\")\n",
    "        for name, description in self.principles.items():\n",
    "            print(f\"  - {name}: {description}\")\n",
    "        \n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        print(\"Using LLM as judge for verification and novelty evaluation\")\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = ConstitutionalEvaluator(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f644a3b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Before you can check accuracy, you need to extract the final numerical answer from each solution. This function uses multiple regex patterns to find answers in various formats.\n",
    "\n",
    "**Challenge**: Mathematical solutions can express answers in many ways - \"The answer is 42\", \"= 42\", \"Total: $42\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ff6ee8b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test extraction: 'Step 1: 5 + 3 = 8. Step 2: 8 * 2 = 16. Therefore, the answer is 16.' -> 16.0\n"
     ]
    }
   ],
   "source": [
    "def extract_numerical_answer(self, solution: str) -> Optional[float]:\n",
    "\n",
    "    #Extract the final numerical answer from a solution using REGEX\n",
    "    patterns = [\n",
    "        r\"(?:The answer is|answer is|Therefore|final answer,?)\\s*\\$?([+-]?\\d+(?:,\\d{3})*(?:\\.\\d+)?)\",\n",
    "        r\"(?:So,?|Thus,?)\\s*.*?\\$?([+-]?\\d+(?:,\\d{3})*(?:\\.\\d+)?)\",\n",
    "        r\"=\\s*\\$?([+-]?\\d+(?:,\\d{3})*(?:\\.\\d+)?)(?:\\s|\\.|\\n|$)\",\n",
    "        r\"(?:got|has|have|total|profit|difference|more|needs?)\\s*\\$?([+-]?\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*(?:more|profit|dollars?|units?|eggs?|oranges?|\\.|\\n|$)\",\n",
    "    ]\n",
    "    \n",
    "    solution_lines = solution.strip().split('\\n')\n",
    "    \n",
    "    # Check last few lines first\n",
    "    for line in reversed(solution_lines[-3:]):\n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, line, re.IGNORECASE)\n",
    "            if matches:\n",
    "                try:\n",
    "                    return float(matches[-1].replace(',', ''))\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    # Fallback: get last number\n",
    "    all_numbers = re.findall(r'([+-]?\\d+(?:,\\d{3})*(?:\\.\\d+)?)', solution)\n",
    "    if all_numbers:\n",
    "        try:\n",
    "            return float(all_numbers[-1].replace(',', ''))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Add method to class\n",
    "ConstitutionalEvaluator.extract_numerical_answer = extract_numerical_answer\n",
    "\n",
    "# Test extraction\n",
    "test_solution = \"Step 1: 5 + 3 = 8. Step 2: 8 * 2 = 16. Therefore, the answer is 16.\"\n",
    "extracted = evaluator.extract_numerical_answer(test_solution)\n",
    "print(f\"Test extraction: '{test_solution}' -> {extracted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c35cbc0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Exercise 3: Implement Accuracy Assessment\n",
    "\n",
    "This function implements the accuracy principle by comparing extracted answers to ground truth values. It uses a small tolerance (0.01) to handle floating-point precision issues.\n",
    "\n",
    "Binary scoring (1.0 for correct, 0.0 for incorrect) ensures accuracy is treated as non-negotiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0803ee7d",
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test: extracted=16.0, ground_truth=16.0, score=1.0\n"
     ]
    }
   ],
   "source": [
    "# GRADED CELL: exercise 3\n",
    "\n",
    "def check_accuracy(self, problem: str, solution: str, ground_truth: float) -> float:\n",
    "    ### START CODE HERE ###\n",
    "    # Use extract_numerical_answer function defined above to extract the numerical answer from the solution\n",
    "    extracted_answer = evaluator.extract_numerical_answer(solution)\n",
    "\n",
    "    # Return 0.0 if extraction failed (if extracted answer is None)\n",
    "    if not extracted_answer:\n",
    "        return 0.0\n",
    "    \n",
    "    # Return 1.0 if answer matches ground truth (within 0.01 tolerance), else 0.0\n",
    "    if abs(extracted_answer - ground_truth) < 0.01:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "# Add method to class\n",
    "ConstitutionalEvaluator.check_accuracy = check_accuracy\n",
    "\n",
    "# Test accuracy checking\n",
    "test_ground_truth = 16.0\n",
    "accuracy_score = evaluator.check_accuracy(\"test problem\", test_solution, test_ground_truth)\n",
    "print(f\"Accuracy test: extracted={extracted}, ground_truth={test_ground_truth}, score={accuracy_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08157f25-433b-4a20-ae41-153757496595",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jp-MarkdownHeadingCollapsed": false
   },
   "source": [
    "## Assessments <a id=\"assessments\"></a>\n",
    "\n",
    "You will investigate the solutions that the model produced and assess them for quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe36728",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Exercise 4: Evaluate Solution Completeness\n",
    "\n",
    "This function evaluates whether solutions show adequate intermediate steps and reasoning. It looks for step indicators, calculations, and explanatory text.\n",
    "\n",
    "Implement a function that takes the solution as input and checks if intermediate steps are shown using REGEX or LLM. Assign credits for different aspects of completeness. Minimum score is 0 and maximum score is 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9728da1",
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completeness test: score=0.6000000000000001\n"
     ]
    }
   ],
   "source": [
    "# GRADED CELL: exercise 4\n",
    "\n",
    "def check_completeness(self, problem: str, solution: str) -> float:\n",
    "    \n",
    "    score = 0.0\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Convert solution to lowercase for easier matching\n",
    "    solution_lower = solution.lower()\n",
    "    \n",
    "    # Check for step indicators (e.g., \"step 1\", \"first\", \"then\", \"calculate\")\n",
    "    # Award up to 0.4 points based on how many step indicators are found\n",
    "    step_indicators = [ \n",
    "        r'step \\d+', r'first', r'second', r'third', r'next', r'then', r'finally', \n",
    "        r'calculate', r'find', r'determine', r'multiply', r'divide', r'add', r'subtract' \n",
    "    ] \n",
    "    \n",
    "    # Count occurrences of step indicators and add to score\n",
    "    # Hint: Use re.findall() for each indicator\n",
    "    step_count = sum(len(re.findall(indicator, solution_lower)) for indicator in step_indicators)\n",
    "    \n",
    "    # If there are 5 or more steps, award 0.4 points.\n",
    "    if step_count >= 5:\n",
    "        score += 0.4\n",
    "    # If there are 3 or more steps, award 0.3 points.\n",
    "    elif step_count >= 3:\n",
    "        score += 0.3\n",
    "    # If there are 1 or more steps, award 0.2 points.\n",
    "    elif step_count >= 1:\n",
    "        score += 0.2\n",
    "    \n",
    "    # Check for intermediate calculations (e.g., \"5 * 3 = None\n",
    "    # Award up to 0.3 points based on number of calculations shown\n",
    "    calculations = re.findall(r'\\d+(?:\\.\\d+)?\\s*[+\\-*/รรท]\\s*\\d+(?:\\.\\d+)?\\s*=\\s*\\d+(?:\\.\\d+)?', solution) \n",
    "    # If there are 3 or more calculations, award 0.3 points.\n",
    "    if len(calculations) >= 3:\n",
    "        score += 0.3\n",
    "    # If there are 1 or more calculations, award 0.2 points.\n",
    "    elif len(calculations) >= 1:\n",
    "        score += 0.2\n",
    "    \n",
    "    \n",
    "    # Check for explanatory phrases (e.g., \"because\", \"therefore\", \"this means\")\n",
    "    # Award up to 0.3 points based on number of explanations\n",
    "    explanation_phrases = ['because', 'since', 'so', 'therefore', 'this means', 'we need to'] \n",
    "    # Count how many explanation phrases appear in the solution\n",
    "    explanation_count = sum(1 for phrase in explanation_phrases if phrase in solution_lower) \n",
    "    # If there are 3 or more explanations, award 0.3 points.\n",
    "    if explanation_count >= 3:\n",
    "        score += 0.3\n",
    "    # If there are 1 or more explanations, award 0.2 points.\n",
    "    elif explanation_count >= 1:\n",
    "        score += 0.2\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Return final score (capped at 1.0)\n",
    "    return min(score, 1.0)\n",
    "\n",
    "# Add method to class\n",
    "ConstitutionalEvaluator.check_completeness = check_completeness\n",
    "\n",
    "# Test completeness\n",
    "completeness_score = evaluator.check_completeness(\"test\", test_solution)\n",
    "print(f\"Completeness test: score={completeness_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5752c6ef",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "For complex judgments like verification quality and solution novelty, you can use the LLM itself as a judge. This helper function handles the LLM evaluation process.\n",
    "\n",
    "**Key Features**: \n",
    "- Low temperature for consistent scoring\n",
    "- Short generation for efficiency\n",
    "- Error handling with fallback scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70923688",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def _get_llm_score(self, prompt: str) -> float:\n",
    "    \"\"\"Helper to get score from LLM\"\"\"\n",
    "    try:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        formatted_prompt = self.tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "        input_ids = inputs.input_ids.to(self.model.device)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=20,\n",
    "                temperature=0.1,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id or self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        input_length = input_ids.shape[1]\n",
    "        generated_tokens = outputs[0][input_length:]\n",
    "        response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "        \n",
    "        numbers = re.findall(r'(\\d*\\.?\\d+)', response)\n",
    "        if numbers:\n",
    "            score = float(numbers[0])\n",
    "            return min(max(score, 0.0), 1.0)\n",
    "        else:\n",
    "            return 0.5\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"LLM evaluation failed: {e}\")\n",
    "        return 0.5\n",
    "\n",
    "# Add method to class\n",
    "ConstitutionalEvaluator._get_llm_score = _get_llm_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55372ff5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Exercise 5: Assess Verification Quality\n",
    "\n",
    "This function uses the LLM to evaluate whether verification solutions include proper answer checking. Human evaluation of verification quality is difficult, so you can delegate this to the LLM.\n",
    "\n",
    "Complete the function below to rate a solution's verification from 0 to 1. Write a prompt and guide the LLM to make consistent verification assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "053e2eff",
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification assessment function defined!\n"
     ]
    }
   ],
   "source": [
    "# GRADED CELL: exercise 5\n",
    "\n",
    "def check_verification(self, problem: str, solution: str) -> float:\n",
    "    ### START CODE HERE ###\n",
    "    # Use LLM to check if solution includes proper verification\n",
    "    # Make sure to pass the {problem} and the {solution} to the prompt.\n",
    "    prompt = \"\"\"You are an expert in assessment. Please assess the quality of verification solution for the given problem.\n",
    "    Problem:\n",
    "    {problem}\n",
    "\n",
    "    Solution:\n",
    "    {solution}\n",
    "    \"\"\"\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return self._get_llm_score(prompt) # @REPLACE return None\n",
    "\n",
    "# Add method to class\n",
    "ConstitutionalEvaluator.check_verification = check_verification\n",
    "print(\"Verification assessment function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ff165b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Exercise 6: Measure Solution Novelty\n",
    " \n",
    "This function evaluates whether alternative solutions use genuinely different approaches compared to the CoT solutions. This is crucial for ensuring solution diversity.\n",
    "\n",
    "Complete the novelty assessment prompt to help the LLM distinguish between truly different approaches and mere rewordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d83ac812",
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED CELL: exercise 6\n",
    "\n",
    "def check_novelty(self, cot_solution: str, alternative_solution: str) -> float:\n",
    "    \"\"\"Use LLM to check if alternative uses genuinely different approach\"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Complete this prompt for novelty assessment\n",
    "    # Make sure to pass both solutions to the prompt, so the model can compare them.\n",
    "    prompt = \"\"\"You are an expert evaluator. \n",
    "    Evaluate whether the given alternative solution uses genuinely different approach compared to the given CoT solution and not mere rewordings.\n",
    "    Assign a score based on novelty and diversity of the alternate solution.\n",
    "    \n",
    "    Alternative solution:\n",
    "    {alternative_solution}\n",
    "\n",
    "    CoT solution:\n",
    "    {cot_solution}\n",
    "    \n",
    "    \"\"\"\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return self._get_llm_score(prompt) # @REPLACE return None\n",
    "\n",
    "# Add method to class\n",
    "ConstitutionalEvaluator.check_novelty = check_novelty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1576fc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "This is the main evaluation function that applies all constitutional principles to rank the three solutions. It uses different scoring weights for different solution types.\n",
    "\n",
    "**Scoring Strategy**:\n",
    "- **CoT**: Accuracy (60%) + Completeness (40%)\n",
    "- **Verification**: Accuracy (50%) + Completeness (30%) + Verification (20%)\n",
    "- **Alternative**: Accuracy (50%) + Completeness (30%) + Novelty (20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6952fcf2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "It should take no more than 60 seconds to run this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe50e0cb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing constitutional evaluation...\n",
      "\n",
      "Evaluating Problem 1...\n",
      "Problem: Maddox and Theo both bought 3 Polaroid Cameras, ea...\n",
      "Ground Truth: 15.0\n",
      "  Rank 1: COT\n",
      "    Composite Score: 0.88\n",
      "    Individual Scores: {'accuracy': 1.0, 'completeness': 0.7}\n",
      "  Rank 2: VERIFICATION\n",
      "    Composite Score: 0.78\n",
      "    Individual Scores: {'accuracy': 1.0, 'completeness': 0.6, 'verification': 0.5}\n",
      "  Rank 3: ALTERNATIVE\n",
      "    Composite Score: 0.31\n",
      "    Individual Scores: {'accuracy': 0.0, 'completeness': 0.7, 'novelty': 0.5}\n",
      "\n",
      "Evaluating Problem 2...\n",
      "\n",
      "Evaluating Problem 3...\n",
      "\n",
      "Evaluating Problem 4...\n",
      "\n",
      "Evaluating Problem 5...\n",
      "\n",
      "Evaluating Problem 6...\n",
      "\n",
      "Evaluating Problem 7...\n",
      "\n",
      "Evaluating Problem 8...\n",
      "\n",
      "Evaluating Problem 9...\n",
      "\n",
      "Evaluating Problem 10...\n",
      "\n",
      "Evaluating Problem 11...\n",
      "\n",
      "Evaluating Problem 12...\n",
      "\n",
      "Evaluating Problem 13...\n",
      "\n",
      "Evaluating Problem 14...\n",
      "\n",
      "Evaluating Problem 15...\n",
      "\n",
      "Evaluating Problem 16...\n",
      "\n",
      "Evaluating Problem 17...\n",
      "\n",
      "Evaluating Problem 18...\n",
      "\n",
      "Evaluating Problem 19...\n",
      "\n",
      "Evaluating Problem 20...\n",
      "\n",
      "Evaluating Problem 21...\n",
      "\n",
      "Evaluating Problem 22...\n",
      "\n",
      "Evaluating Problem 23...\n",
      "\n",
      "Evaluating Problem 24...\n",
      "\n",
      "Evaluating Problem 25...\n",
      "\n",
      "Evaluating Problem 26...\n",
      "\n",
      "Evaluating Problem 27...\n",
      "\n",
      "Evaluating Problem 28...\n",
      "\n",
      "Evaluating Problem 29...\n",
      "\n",
      "Evaluating Problem 30...\n",
      "\n",
      "Evaluating Problem 31...\n",
      "\n",
      "Evaluating Problem 32...\n",
      "\n",
      "Evaluating Problem 33...\n",
      "\n",
      "Evaluating Problem 34...\n",
      "\n",
      "Evaluating Problem 35...\n",
      "\n",
      "Evaluating Problem 36...\n",
      "\n",
      "Evaluating Problem 37...\n",
      "\n",
      "Evaluating Problem 38...\n",
      "\n",
      "Evaluating Problem 39...\n",
      "\n",
      "Evaluating Problem 40...\n",
      "\n",
      "Evaluating Problem 41...\n",
      "\n",
      "Evaluating Problem 42...\n",
      "\n",
      "Evaluating Problem 43...\n",
      "\n",
      "Evaluating Problem 44...\n",
      "\n",
      "Evaluating Problem 45...\n",
      "\n",
      "Evaluating Problem 46...\n",
      "\n",
      "Evaluating Problem 47...\n",
      "\n",
      "Evaluating Problem 48...\n",
      "\n",
      "Evaluating Problem 49...\n",
      "\n",
      "Evaluating Problem 50...\n",
      "Results saved to evaluation_results.json\n",
      "\n",
      "Evaluation complete! Results saved to evaluation_results.json\n",
      "Evaluation Results Summary\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Problem</th>\n",
       "      <th>Ground Truth</th>\n",
       "      <th>Template</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Composite Score</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>completeness</th>\n",
       "      <th>verification</th>\n",
       "      <th>novelty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>COT</td>\n",
       "      <td>1</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>VERIFICATION</td>\n",
       "      <td>2</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>ALTERNATIVE</td>\n",
       "      <td>3</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>COT</td>\n",
       "      <td>1</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>ALTERNATIVE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>VERIFICATION</td>\n",
       "      <td>3</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>90.0</td>\n",
       "      <td>COT</td>\n",
       "      <td>1</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>90.0</td>\n",
       "      <td>VERIFICATION</td>\n",
       "      <td>2</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>90.0</td>\n",
       "      <td>ALTERNATIVE</td>\n",
       "      <td>3</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>96.0</td>\n",
       "      <td>COT</td>\n",
       "      <td>1</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>96.0</td>\n",
       "      <td>VERIFICATION</td>\n",
       "      <td>2</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>96.0</td>\n",
       "      <td>ALTERNATIVE</td>\n",
       "      <td>3</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>COT</td>\n",
       "      <td>1</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>ALTERNATIVE</td>\n",
       "      <td>2</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>VERIFICATION</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6</td>\n",
       "      <td>24.0</td>\n",
       "      <td>COT</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6</td>\n",
       "      <td>24.0</td>\n",
       "      <td>VERIFICATION</td>\n",
       "      <td>2</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>24.0</td>\n",
       "      <td>ALTERNATIVE</td>\n",
       "      <td>3</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7</td>\n",
       "      <td>48.0</td>\n",
       "      <td>COT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7</td>\n",
       "      <td>48.0</td>\n",
       "      <td>VERIFICATION</td>\n",
       "      <td>2</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Problem  Ground Truth      Template  Rank  Composite Score  accuracy  \\\n",
       "0         1          15.0           COT     1             0.88       1.0   \n",
       "1         1          15.0  VERIFICATION     2             0.78       1.0   \n",
       "2         1          15.0   ALTERNATIVE     3             0.31       0.0   \n",
       "3         2           9.0           COT     1             0.88       1.0   \n",
       "4         2           9.0   ALTERNATIVE     2             0.87       1.0   \n",
       "5         2           9.0  VERIFICATION     3             0.81       1.0   \n",
       "6         3          90.0           COT     1             0.96       1.0   \n",
       "7         3          90.0  VERIFICATION     2             0.84       1.0   \n",
       "8         3          90.0   ALTERNATIVE     3             0.81       1.0   \n",
       "9         4          96.0           COT     1             0.96       1.0   \n",
       "10        4          96.0  VERIFICATION     2             0.87       1.0   \n",
       "11        4          96.0   ALTERNATIVE     3             0.72       1.0   \n",
       "12        5        1260.0           COT     1             0.84       1.0   \n",
       "13        5        1260.0   ALTERNATIVE     2             0.84       1.0   \n",
       "14        5        1260.0  VERIFICATION     3             0.75       1.0   \n",
       "15        6          24.0           COT     1             0.92       1.0   \n",
       "16        6          24.0  VERIFICATION     2             0.87       1.0   \n",
       "17        6          24.0   ALTERNATIVE     3             0.72       1.0   \n",
       "18        7          48.0           COT     1             1.00       1.0   \n",
       "19        7          48.0  VERIFICATION     2             0.81       1.0   \n",
       "\n",
       "    completeness  verification  novelty  \n",
       "0            0.7           NaN      NaN  \n",
       "1            0.6           0.5      NaN  \n",
       "2            0.7           NaN      0.5  \n",
       "3            0.7           NaN      NaN  \n",
       "4            0.9           NaN      0.5  \n",
       "5            0.7           0.5      NaN  \n",
       "6            0.9           NaN      NaN  \n",
       "7            0.8           0.5      NaN  \n",
       "8            0.7           NaN      0.5  \n",
       "9            0.9           NaN      NaN  \n",
       "10           0.9           0.5      NaN  \n",
       "11           0.4           NaN      0.5  \n",
       "12           0.6           NaN      NaN  \n",
       "13           0.8           NaN      0.5  \n",
       "14           0.5           0.5      NaN  \n",
       "15           0.8           NaN      NaN  \n",
       "16           0.9           0.5      NaN  \n",
       "17           0.4           NaN      0.5  \n",
       "18           1.0           NaN      NaN  \n",
       "19           0.7           0.5      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate_solutions(self, problem: str, ground_truth: float, solutions: List[Dict]) -> Dict:\n",
    "    \"\"\"Evaluate all three solutions and return ranked results\"\"\"\n",
    "    evaluated_solutions = []\n",
    "    \n",
    "    for solution in solutions:\n",
    "        template = solution['template']\n",
    "        response = solution['response']\n",
    "        \n",
    "        accuracy = self.check_accuracy(problem, response, ground_truth)\n",
    "        completeness = self.check_completeness(problem, response)\n",
    "        \n",
    "        if template == 'verification':\n",
    "            verification = self.check_verification(problem, response)\n",
    "            scores = {\n",
    "                'accuracy': round(accuracy, 3),\n",
    "                'completeness': round(completeness, 3),\n",
    "                'verification': round(verification, 3)\n",
    "            }\n",
    "            composite_score = 0.5 * accuracy + 0.3 * completeness + 0.2 * verification\n",
    "        \n",
    "        elif template == 'alternative':\n",
    "            cot_solution = next((s['response'] for s in solutions if s['template'] == 'cot'), \"\")\n",
    "            novelty = self.check_novelty(cot_solution, response)\n",
    "            scores = {\n",
    "                'accuracy': round(accuracy, 3),\n",
    "                'completeness': round(completeness, 3),\n",
    "                'novelty': round(novelty, 3)\n",
    "            }\n",
    "            composite_score = 0.5 * accuracy + 0.3 * completeness + 0.2 * novelty\n",
    "        \n",
    "        else:  # CoT template\n",
    "            scores = {\n",
    "                'accuracy': round(accuracy, 3),\n",
    "                'completeness': round(completeness, 3)\n",
    "            }\n",
    "            composite_score = 0.6 * accuracy + 0.4 * completeness\n",
    "        \n",
    "        evaluated_solutions.append({\n",
    "            'template': template,\n",
    "            'scores': scores,\n",
    "            'composite_score': round(composite_score, 3)\n",
    "        })\n",
    "    \n",
    "    evaluated_solutions.sort(key=lambda x: x['composite_score'], reverse=True)\n",
    "    \n",
    "    result = {\n",
    "        'problem': problem,\n",
    "        'ground_truth': ground_truth,\n",
    "        'solutions': []\n",
    "    }\n",
    "    \n",
    "    for i, solution in enumerate(evaluated_solutions):\n",
    "        solution['rank'] = i + 1\n",
    "        result['solutions'].append(solution)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Add method to class\n",
    "ConstitutionalEvaluator.evaluate_solutions = evaluate_solutions\n",
    "\n",
    "# Test evaluation on generated solutions\n",
    "print(\"Testing constitutional evaluation...\")\n",
    "evaluation_results = []\n",
    "\n",
    "for i, result in enumerate(solution_results):\n",
    "    print(f\"\\nEvaluating Problem {i+1}...\")\n",
    "    \n",
    "    evaluation = evaluator.evaluate_solutions(\n",
    "        result['question'],\n",
    "        float(result['ground_truth_answer']),\n",
    "        result['solutions']\n",
    "    )\n",
    "    evaluation_results.append(evaluation)\n",
    "    \n",
    "    # Display results for first problem\n",
    "    if i == 0:\n",
    "        print(f\"Problem: {result['question'][:50]}...\")\n",
    "        print(f\"Ground Truth: {result['ground_truth_answer']}\")\n",
    "        for solution in evaluation['solutions']:\n",
    "            print(f\"  Rank {solution['rank']}: {solution['template'].upper()}\")\n",
    "            print(f\"    Composite Score: {solution['composite_score']}\")\n",
    "            print(f\"    Individual Scores: {solution['scores']}\")\n",
    "\n",
    "# Save evaluation results\n",
    "save_results(evaluation_results, \"evaluation_results.json\")\n",
    "print(f\"\\nEvaluation complete! Results saved to evaluation_results.json\")\n",
    "df = display_evaluation_results(\"evaluation_results.json\", num_rows=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b1fb2-33dd-47f1-9cfa-f238f53010ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
