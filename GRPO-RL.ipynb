{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Graded Lab: GRPO Post Training Lab\n",
    "\n",
    "Welcome to the second assignment of this module!\n",
    "\n",
    "Carefully read each Markdown (text) cell, which include instructions and hints. Start by reading the background behind your upcoming tasks.\n",
    "\n",
    "When you are done, submit your solution by saving it, then clicking on the submit button at the top right side of the page.\n",
    "\n",
    "## In order for your submission to be graded correctly, you **MUST**:\n",
    "* **Use the provided variable names**, otherwise the autograder will not be able to locate the variable for grading. \n",
    "\n",
    "* **Replace any instances of `None` with your own code.** \n",
    "\n",
    "* **Only modify the cells that start with the comment `# GRADED CELL`**.  \n",
    "\n",
    "* **Use the provided cells for your solution.** You can add new cells to experiment, but these will be omitted when grading. \n",
    "\n",
    "To submit your solution, save it, then click on the blue submit button at the top of the page.\n",
    "\n",
    "<div style=\"background-color: #FAD888; padding: 10px; border-radius: 3px; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1); width:95%\n",
    "\">\n",
    "<strong>Important notes</strong>:\n",
    "\n",
    "- Code blocks with None will not run properly. If you run them before completing the exercise, you will likely get an error. \n",
    "\n",
    "- The notebooks work best in Chrome browser. If you are having problems, please switch to Chrome.\n",
    "\n",
    "- Make sure you always save before submitting.\n",
    "</div>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this hands-on tutorial, you'll learn how to improve a Large Language Model's ability to solve math problems using a technique called **GRPO**. This involves generating multiple answers to the same question, comparing answers to find which are better, and teaching the model to prefer better approaches.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will build a comprehensive reward system for training a Large Language Model to solve math problems using GRPO (Group Relative Policy Optimization). This involves creating sophisticated reward functions that can evaluate model responses across different quality levels and implementing a complete GRPO training pipeline.\n",
    "\n",
    "* **Extract Numerical Answers from Model Responses:** Implement robust parsing to extract numerical answers from various response formats including GSM8K standard format and common answer phrases.\n",
    "* **Analyze Response Quality Indicators:** Build a quality analysis system that detects mathematical reasoning, step-by-step thinking, and structured solutions in model responses.\n",
    "* **Reward Unparseable Responses:** Create a reward system for responses without clear numerical answers, based on effort and reasoning quality.\n",
    "* **Reward High-Quality Correct Answers:** Implement a bonus system that encourages not just correctness but also clear mathematical communication and detailed explanations.\n",
    "* **Implement Partial Credit for Wrong Answers:** Develop a partial credit system that provides learning gradients for wrong answers based on proximity to correct solutions and quality of reasoning shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Setup](#setup)\n",
    "* [Training Configuration](#trainingconfiguration)\n",
    "* [Load the GSM8K dataset](#loadGSM8K)\n",
    "* [Create the Reward Function](#createrewardfunction) - Exercise 1, 2, 3, 4, 5\n",
    "* [Load the Language Model](#loadthelanguagemodel)\n",
    "* [Prepare Training and Validation datasets](#preparetraining)\n",
    "* [Create Evaluation Callback](#createevaluation)\n",
    "* [Configure GRPO Trainer](#configuregrpotrainer)\n",
    "* [Train the Model with GRPO! (Ungraded Part)](#trainmodelwithgrpo)\n",
    "* [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Setup <a id=\"setup\"></a>\n",
    "\n",
    "Start by importing all the necessary packages, setting up random seeds for reproducibility, setting up the devices and the logger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logging configured - all detailed logs will be written to: ./grpo_logs/grpo_20251105_052057.log\n",
      "Note: Detailed reward computation logs will only appear in the log file, not in console output\n",
      "‚úÖ libraries imported\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "# Disable progress bars to avoid Jupyter context errors\n",
    "os.environ['HF_DATASETS_DISABLE_PROGRESS_BAR'] = '1'\n",
    "import re          \n",
    "import random      \n",
    "import logging     \n",
    "import warnings    \n",
    "from typing import List, Dict, Optional, Tuple  \n",
    "from dataclasses import dataclass, field  \n",
    "\n",
    "import numpy as np  \n",
    "import torch \n",
    "from trl import (\n",
    "    GRPOConfig,   \n",
    "    GRPOTrainer \n",
    ")\n",
    "from transformers import (\n",
    "    AutoTokenizer,  \n",
    "    AutoModelForCausalLM,    \n",
    ")\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "\n",
    "from utils import (\n",
    "    setup_logging,  \n",
    "    load_and_explore_gsm8k_dataset, \n",
    "    prepare_dataset, \n",
    "    GSM8KEvaluationCallback, \n",
    "    evaluate_and_compare              \n",
    ")\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "random.seed(42)    \n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger, log_file = setup_logging(device=device)\n",
    "\n",
    "print(\"‚úÖ libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Training Configuration <a id=\"trainingconfiguration\"></a>\n",
    "\n",
    "You'll configure all your training settings in one place. This makes it easy to experiment with different values.\n",
    "\n",
    "Important Trade-offs:\n",
    "\n",
    "- **Higher batch size** = More stable but needs more memory\n",
    "- **More generations** = Better comparison but slower training\n",
    "- **Higher learning rate** = Faster learning but might \"overshoot\"\n",
    "- **More epochs** = More learning but might overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration class defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Define Training Configuration Class\n",
    "# ============================================\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"\n",
    "    Configuration for GRPO training.\n",
    "    \n",
    "    This class holds all the settings (hyperparameters) for training.\n",
    "    Think of it as a control panel with all the knobs and switches.\n",
    "    \n",
    "    Each parameter has:\n",
    "    - A default value (recommended value)\n",
    "    - A description (what it does)\n",
    "    - A type (what kind of value it expects)\n",
    "    \"\"\"\n",
    "    \n",
    "    # ========== MODEL SETTINGS ==========\n",
    "    # Which model to use and where to save it\n",
    "    \n",
    "    model_name: str = field(\n",
    "        default=\"/app/models/deepseek-math-7b-base\",\n",
    "        metadata={\"help\": \"The pre-trained model to start with.\"}\n",
    "    )\n",
    "    \n",
    "    output_dir: str = field(\n",
    "        default=\"./grpo_finetuned_model\",\n",
    "        metadata={\"help\": \"Where to save the trained model. Like a 'Save As' location.\"}\n",
    "    )\n",
    "    \n",
    "    # ========== TRAINING DURATION ==========\n",
    "    # How long to train for\n",
    "    \n",
    "    num_train_epochs: int = field(\n",
    "        default=5,\n",
    "        metadata={\"help\": \"How many times to go through the training data. More = more learning.\"}\n",
    "    )\n",
    "    \n",
    "    # ========== BATCH SETTINGS ==========\n",
    "    # How many examples to process at once\n",
    "    \n",
    "    per_device_train_batch_size: int = field(\n",
    "        default=2,\n",
    "        metadata={\"help\": \"How many problems to process at once. Limited by GPU memory.\"}\n",
    "    )\n",
    "    \n",
    "    gradient_accumulation_steps: int = field(\n",
    "        default=32,\n",
    "        metadata={\"help\": \"Accumulate gradients over multiple batches. Simulates larger batch size.\"}\n",
    "    )\n",
    "    # Effective batch size = per_device_train_batch_size * gradient_accumulation_steps = 64\n",
    "    \n",
    "    # ========== LEARNING SETTINGS ==========\n",
    "    # How fast the model learns\n",
    "    \n",
    "    learning_rate: float = field(\n",
    "        default=5e-6,  # 0.000005 in decimal\n",
    "        metadata={\"help\": \"How big of a step to take when learning. Too high = unstable.\"}\n",
    "    )\n",
    "    \n",
    "    # ========== GRPO SPECIFIC SETTINGS ==========\n",
    "    # Settings unique to GRPO algorithm\n",
    "    \n",
    "    num_generations: int = field(\n",
    "        default=12,\n",
    "        metadata={\"help\": \"How many different answers to generate per question. More = better comparison.\"}\n",
    "    )\n",
    "    \n",
    "    temperature: float = field(\n",
    "        default=0.8,\n",
    "        metadata={\"help\": \"Controls randomness. 0.0 = always same answer, 1.0 = very random.\"}\n",
    "    )\n",
    "    \n",
    "    max_new_tokens: int = field(\n",
    "        default=400,\n",
    "        metadata={\"help\": \"Maximum length of generated answers. Needs to be long enough for full solutions.\"}\n",
    "    )\n",
    "    \n",
    "    # ========== DATA SETTINGS ==========\n",
    "    # How to handle the dataset\n",
    "    \n",
    "    max_prompt_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\"help\": \"Maximum length of input questions in tokens.\"}\n",
    "    )\n",
    "    \n",
    "    train_split_ratio: float = field(\n",
    "        default=0.8,\n",
    "        metadata={\"help\": \"What fraction of data to use for training (rest for validation).\"}\n",
    "    )\n",
    "    \n",
    "    # ========== MONITORING SETTINGS ==========\n",
    "    # How often to check progress\n",
    "    \n",
    "    eval_steps: int = field(\n",
    "        default=20,\n",
    "        metadata={\"help\": \"Evaluate model every N steps to check progress.\"}\n",
    "    )\n",
    "    \n",
    "    save_steps: int = field(\n",
    "        default=20,\n",
    "        metadata={\"help\": \"Save a checkpoint every N steps (for recovery if training stops).\"}\n",
    "    )\n",
    "    \n",
    "    logging_steps: int = field(\n",
    "        default=20,\n",
    "        metadata={\"help\": \"Log training metrics every N steps.\"}\n",
    "    )\n",
    "    \n",
    "    save_total_limit: int = field(\n",
    "        default=3,\n",
    "        metadata={\"help\": \"Keep only the N most recent checkpoints to save disk space.\"}\n",
    "    )\n",
    "    \n",
    "    # ========== OTHER SETTINGS ==========\n",
    "    \n",
    "    seed: int = field(\n",
    "        default=42,\n",
    "        metadata={\"help\": \"Random seed for reproducibility.\"}\n",
    "    )\n",
    "    \n",
    "    use_8bit: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Load model in 8-bit mode to save memory (slightly less accurate).\"}\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Configuration class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING CONFIGURATION\n",
      "==================================================\n",
      "\n",
      "Model Settings:\n",
      "  Model: /app/models/deepseek-math-7b-base\n",
      "  Output directory: ./grpo_finetuned_model\n",
      "\n",
      "Training Duration:\n",
      "  Epochs: 5\n",
      "  Batch size per device: 2\n",
      "  Gradient accumulation: 32\n",
      "  Effective batch size: 64\n",
      "\n",
      "GRPO Settings:\n",
      "  Generations per prompt: 12\n",
      "  Temperature: 0.8\n",
      "  Max new tokens: 400\n",
      "  Learning rate: 5e-06\n",
      "\n",
      "Monitoring:\n",
      "  Evaluate every: 20 steps\n",
      "  Save every: 20 steps\n",
      "  Log every: 20 steps\n",
      "\n",
      "Estimated Training Info:\n",
      "  This configuration will generate 12 answers per question\n",
      "  The model will learn by comparing these answers\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Create and Display Configuration\n",
    "# ============================================\n",
    "\n",
    "# Create an instance of your configuration\n",
    "# This uses all the default values defined above\n",
    "config = TrainingConfig()\n",
    "\n",
    "# Display all configuration values\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Group settings by category for easier reading\n",
    "print(\"\\nModel Settings:\")\n",
    "print(f\"  Model: {config.model_name}\")\n",
    "print(f\"  Output directory: {config.output_dir}\")\n",
    "\n",
    "print(\"\\nTraining Duration:\")\n",
    "print(f\"  Epochs: {config.num_train_epochs}\")\n",
    "print(f\"  Batch size per device: {config.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {config.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {config.per_device_train_batch_size * config.gradient_accumulation_steps}\")\n",
    "\n",
    "print(\"\\nGRPO Settings:\")\n",
    "print(f\"  Generations per prompt: {config.num_generations}\")\n",
    "print(f\"  Temperature: {config.temperature}\")\n",
    "print(f\"  Max new tokens: {config.max_new_tokens}\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "\n",
    "print(\"\\nMonitoring:\")\n",
    "print(f\"  Evaluate every: {config.eval_steps} steps\")\n",
    "print(f\"  Save every: {config.save_steps} steps\")\n",
    "print(f\"  Log every: {config.logging_steps} steps\")\n",
    "\n",
    "# Calculate approximate training time\n",
    "print(\"\\nEstimated Training Info:\")\n",
    "print(f\"  This configuration will generate {config.num_generations} answers per question\")\n",
    "print(f\"  The model will learn by comparing these answers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Load the GSM8K Dataset <a id=\"loadGSM8K\"></a>\n",
    "\n",
    "Here you will load the GSM8K dataset, which you are already familiar with from the previous assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 7473\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 1319\n",
      "    })\n",
      "})\n",
      "\n",
      "Dataset splits:\n",
      "- Train: 7473 examples\n",
      "- Test: 1319 examples\n",
      "\n",
      "============================================================\n",
      "Sample Problem from Training Set:\n",
      "============================================================\n",
      "\n",
      "üìù Question:\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "\n",
      "‚úÖ Answer:\n",
      "Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
      "#### 72\n",
      "\n",
      "üî¢ Numerical Answer: 72\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Load and Explore GSM8K Dataset\n",
    "# ============================================\n",
    "\n",
    "# Load the GSM8K dataset\n",
    "# This function:\n",
    "# 1. Downloads the dataset (if not already downloaded)\n",
    "# 2. Shows dataset statistics\n",
    "# 3. Displays sample problems\n",
    "dataset = load_and_explore_gsm8k_dataset()\n",
    "\n",
    "# The dataset has two parts:\n",
    "# - 'train': Problems for training (about 7,500)\n",
    "# - 'test': Problems for testing (about 1,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Create the Reward Function <a id=\"createrewardfunction\"></a>\n",
    "\n",
    "How It Works\n",
    "\n",
    "1. **Extract** the numerical answer from the text\n",
    "2. **Compare** with the correct answer\n",
    "3. **Check** for partial credit (showing work, being close)\n",
    "4. **Assign** a reward score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GSM8KRewardSignal class definition\n",
    "class GSM8KRewardSignal:\n",
    "    \"\"\"\n",
    "    BASELINE Reward Model for GSM8K (Simplified Version)\n",
    "    \n",
    "    This is a basic reward model with only 3 categories:\n",
    "    - Correct answer: 1.0\n",
    "    - Wrong answer: 0.0  \n",
    "    - Unparseable: 0.0\n",
    "    \n",
    "    ‚ö†Ô∏è LIMITATIONS: This simple reward signal makes GRPO training difficult!\n",
    "    The lack of partial credit means the model gets no signal for improvements.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Exercise 1: Extract Numerical Answer from Model Response\n",
    "\n",
    "The first critical component of your reward model is the ability to extract numerical answers from the model's generated text. Language models often produce verbose responses with explanations, calculations, and natural language, but you need to identify and extract the actual numerical answer to compare it with the ground truth.\n",
    "\n",
    "Currently, the basic implementation only looks for answers in the GSM8K standard format (marked with ####). However, models don't always follow this format perfectly. Your task is to implement a more robust extraction system that can handle various answer formats that models might produce.\n",
    "\n",
    "Consider implementing patterns to match common answer phrases like \"The answer is X\", \"equals X\", \"total is X\", or \"Therefore, X\". Remember to handle edge cases such as numbers with commas (e.g., 1,000), dollar signs, negative numbers, and decimal points (you can use this regular expression: `r'([+-]?\\d+(?:,\\d{3})*(?:\\.\\d+)?)'`). As a fallback strategy, you might want to extract the last number mentioned in the response, but make sure your regex pattern is sophisticated enough to properly identify valid numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# GRADED CELL: exercise 1\n",
    " \n",
    "def extract_numerical_answer(self, text: str) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Extract the numerical answer from model's response.\n",
    "\n",
    "    This function looks for numbers in different formats:\n",
    "    - #### 42 (GSM8K format)\n",
    "    - \"The answer is 42\"\n",
    "    - \"= 42\"\n",
    "    - Just finds the last number if nothing else works\n",
    "\n",
    "    Args:\n",
    "        text: The model's generated response\n",
    "\n",
    "    Returns:\n",
    "        The extracted number, or None if no number found\n",
    "    \"\"\"\n",
    "\n",
    "    # First, check for GSM8K format (#### answer)\n",
    "    if \"####\" in text:\n",
    "        answer = text.split(\"####\")[-1].strip()\n",
    "        answer = answer.replace(',', '').replace('$', '')\n",
    "        try:\n",
    "            return float(answer)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Try various answer patterns\n",
    "    # Note: \\$ in regex matches a literal dollar sign\n",
    "    patterns = [\n",
    "        r\"(?:The answer is|answer:|Answer:)\\s*\\$?([+-]?\\d+(?:,\\d{3})*(?:\\.\\d+)?)\",\n",
    "        r\"(?:equals?|=)\\s*\\$?([+-]?\\d+(?:,\\d{3})*(?:\\.\\d+)?)\",\n",
    "        r\"(?:total|sum|result|Total|Final answer)\\s*(?:is|:|=)?\\s*\\$?([+-]?\\d+(?:,\\d{3})*(?:\\.\\d+)?)\",\n",
    "        r\"Therefore,?\\s*\\$?([+-]?\\d+(?:,\\d{3})*(?:\\.\\d+)?)\",\n",
    "    ]\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    # Use the patterns list defined above to find patterns like \"The answer is X\", \"equals X\", \"total is X\", \"Therefore, X\" etc-\n",
    "\n",
    "    # For each of the above patterns\n",
    "    for pattern in patterns:\n",
    "        # Use re.findall() with to match different answer formats\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)\n",
    "        if matches:\n",
    "            try:\n",
    "                # Remove commas and dollar signs and convert to float\n",
    "                return float(matches[-1].replace(',','').replace('$',''))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # Last resort: find any number in the text\n",
    "    # Update this simple regex to handle negative numbers and decimal points\n",
    "    # As a fallback, find the last number in the text using regex\n",
    "    numbers = re.findall(r'-?\\d+(?:\\.\\d+)?', text)\n",
    "    if numbers:\n",
    "        try:\n",
    "            return float(numbers[-1].replace(',', '')) \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return None\n",
    "\n",
    "# Add the method to GSM8KRewardSignal class for grading purposes\n",
    "GSM8KRewardSignal.extract_numerical_answer = extract_numerical_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Exercise 1: Extract Numerical Answer\n",
      "==================================================\n",
      "‚úÖ PASSED: GSM8K standard format\n",
      "‚úÖ PASSED: Common phrase: 'The answer is'\n",
      "‚úÖ PASSED: Common phrase: 'Answer:'\n",
      "‚úÖ PASSED: Common phrase: 'equals'\n",
      "‚úÖ PASSED: Common phrase: 'total is'\n",
      "‚úÖ PASSED: Common phrase: 'Therefore,'\n",
      "‚úÖ PASSED: Dollar sign and commas\n",
      "‚úÖ PASSED: Negative number\n",
      "‚úÖ PASSED: Small decimal\n",
      "‚úÖ PASSED: Last number fallback\n",
      "‚úÖ PASSED: Last number in text\n",
      "\n",
      "==================================================\n",
      "Results: 11/11 passed, 0/11 failed\n",
      "üéâ All tests passed! Exercise 1 is complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# UNIT TEST: Exercise 1 - Extract Numerical Answer\n",
    "# ============================================\n",
    "\n",
    "def test_extract_numerical_answer():\n",
    "    \"\"\"Unit test for the extract_numerical_answer method.\"\"\"\n",
    "    print(\"üß™ Testing Exercise 1: Extract Numerical Answer\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create an instance of the reward model\n",
    "    test_model = GSM8KRewardSignal()\n",
    "    \n",
    "    # Test cases: (input_text, expected_output, description)\n",
    "    test_cases = [\n",
    "        # Standard GSM8K format (already works in base implementation)\n",
    "        (\"The calculation is 5 + 3 = 8 #### 8\", 8.0, \"GSM8K standard format\"),\n",
    "        \n",
    "        # Common answer phrases (MUST work after student implementation)\n",
    "        (\"The answer is 42\", 42.0, \"Common phrase: 'The answer is'\"),\n",
    "        (\"Answer: 100\", 100.0, \"Common phrase: 'Answer:'\"),\n",
    "        (\"equals 25\", 25.0, \"Common phrase: 'equals'\"),\n",
    "        (\"total is 15.5\", 15.5, \"Common phrase: 'total is'\"),\n",
    "        (\"Therefore, 7\", 7.0, \"Common phrase: 'Therefore,'\"),\n",
    "        \n",
    "        # Numbers with formatting (MUST handle these)\n",
    "        (\"The answer is $1,234.56\", 1234.56, \"Dollar sign and commas\"),\n",
    "        (\"Total: -45\", -45.0, \"Negative number\"),\n",
    "        (\"equals 0.003\", 0.003, \"Small decimal\"),\n",
    "        \n",
    "        # Fallback to last number (minimum requirement)\n",
    "        (\"First we have 10, then 20, finally 30\", 30.0, \"Last number fallback\"),\n",
    "        (\"No clear answer but mentions 99\", 99.0, \"Last number in text\"),\n",
    "    ]\n",
    "    \n",
    "    passed = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for text, expected, description in test_cases:\n",
    "        try:\n",
    "            result = test_model.extract_numerical_answer(text)\n",
    "            \n",
    "            if result is None and expected is not None:\n",
    "                print(f\"‚ùå FAILED: {description}\")\n",
    "                print(f\"   Input: '{text}'\")\n",
    "                print(f\"   Expected: {expected}, Got: None\")\n",
    "                failed += 1\n",
    "            elif result is not None and expected is None:\n",
    "                print(f\"‚ùå FAILED: {description}\")\n",
    "                print(f\"   Input: '{text}'\")\n",
    "                print(f\"   Expected: None, Got: {result}\")\n",
    "                failed += 1\n",
    "            elif result is not None and abs(result - expected) > 0.01:\n",
    "                print(f\"‚ùå FAILED: {description}\")\n",
    "                print(f\"   Input: '{text}'\")\n",
    "                print(f\"   Expected: {expected}, Got: {result}\")\n",
    "                failed += 1\n",
    "            else:\n",
    "                print(f\"‚úÖ PASSED: {description}\")\n",
    "                passed += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ERROR: {description}\")\n",
    "            print(f\"   Exception: {e}\")\n",
    "            failed += 1\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Results: {passed}/{len(test_cases)} passed, {failed}/{len(test_cases)} failed\")\n",
    "    \n",
    "    if failed == 0:\n",
    "        print(\"üéâ All tests passed! Exercise 1 is complete.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Some tests failed. Please review your implementation.\")\n",
    "        print(\"\\nMinimum requirements:\")\n",
    "        print(\"- Extract answers from common phrases (The answer is, equals, total is)\")\n",
    "        print(\"- Handle numbers with commas and dollar signs\")\n",
    "        print(\"- Support negative numbers and decimals\")\n",
    "        print(\"- Fallback to extracting the last number in text\")\n",
    "    \n",
    "    return passed == len(test_cases)\n",
    "\n",
    "# Run the test\n",
    "test_extract_numerical_answer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Exercise 2: Analyze Response Quality Indicators\n",
    "\n",
    "Before assigning rewards to model responses, you need to understand what makes a good mathematical solution. This exercise focuses on building a comprehensive quality analysis system that examines various aspects of the generated response.\n",
    "\n",
    "Your task is to implement quality indicators that can detect whether a response contains mathematical reasoning. The current implementation has basic checks, but you should enhance it to identify mathematical operations (not just symbols like +, -, *, / but also words like \"multiply\", \"divide\", \"add\", \"subtract\"), reasoning indicators (words like \"first\", \"then\", \"next\", \"finally\" that show step-by-step thinking), and structured solutions (numbered steps or bullet points).\n",
    "\n",
    "Additionally, consider counting meaningful metrics like the number of sentences (which might indicate detailed explanation), checking for the presence of numbers (essential for math problems), and measuring response length. These quality indicators will be crucial for the reward functions in the following exercises, as they help distinguish between different types of responses even when you can't extract a final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# GRADED CELL: exercise 2\n",
    "\n",
    "def analyze_response_quality(self, response: str) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Analyze the quality indicators of a response.\n",
    "\n",
    "    This method examines various aspects of the response to determine\n",
    "    its quality, even if the final answer is wrong or unparseable.\n",
    "\n",
    "    Args:\n",
    "        response: The model's generated response\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with quality indicators:\n",
    "        - has_calculation: Boolean, if response contains math operations\n",
    "        - has_steps: Boolean, if response shows multiple steps\n",
    "        - has_reasoning: Boolean, if response uses reasoning words\n",
    "        - has_numbers: Boolean, if response contains numbers\n",
    "        - response_length: Int, character count\n",
    "        - sentence_count: Int, number of sentences\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### \n",
    "\n",
    "    # Check for calculations (math operators and related words)\n",
    "    calculation_indicators = ['=', '+', '-', '*', '/', 'multiply', 'divide', 'add', 'subtract'] \n",
    "    # set has_calculation to True if any of the indicators are in response.lower()\n",
    "    has_calculation = any(calc_ind in response.lower() for calc_ind in calculation_indicators)\n",
    "\n",
    "    # Check for steps (multiple sentences or explicit step markers)\n",
    "    sentences = [s.strip() for s in re.split(r'[.!?]+', response) if s.strip()] \n",
    "    sentence_count = len(sentences)\n",
    "    # Set has_steps to true if there are at least two sentences or if the word 'step' is in response.lower()\n",
    "    has_steps = True if (sentence_count >= 2 or 'step' in response.lower()) else False\n",
    "\n",
    "    # Check for reasoning words\n",
    "    # These words indicate logical reasoning or step-by-step thinking\n",
    "    reasoning_words = ['therefore', 'because', 'since', 'so', 'thus', 'hence', \n",
    "                        'first', 'then', 'next', 'finally', 'must', 'need'] \n",
    "    # Set has_reasoning to True if ANY of these reasoning words appear in the response\n",
    "    # Use any() with a generator expression to check if any word is in response.lower()\n",
    "    has_reasoning = any(word in response.lower() for word in reasoning_words)\n",
    "\n",
    "    # Check for numbers\n",
    "    has_numbers = bool(re.search(r'\\d', response)) \n",
    "\n",
    "    # Response length\n",
    "    response_length = len(response)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    quality = {\n",
    "        'has_calculation': has_calculation,\n",
    "        'has_steps': has_steps,\n",
    "        'has_reasoning': has_reasoning,\n",
    "        'has_numbers': has_numbers,\n",
    "        'response_length': response_length,\n",
    "        'sentence_count': sentence_count\n",
    "    }\n",
    "\n",
    "    return quality\n",
    "\n",
    "# Add the method to GSM8KRewardSignal class for grading purposes\n",
    "GSM8KRewardSignal.analyze_response_quality = analyze_response_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Exercise 2: Analyze Response Quality\n",
      "==================================================\n",
      "‚úÖ PASSED: Response with math operations and reasoning\n",
      "‚úÖ PASSED: Single number response\n",
      "‚úÖ PASSED: Numbered steps with calculations\n",
      "‚úÖ PASSED: Give-up response\n",
      "‚úÖ PASSED: Word-based math operations\n",
      "\n",
      "==================================================\n",
      "Results: 5/5 passed, 0/5 failed\n",
      "üéâ All tests passed! Exercise 2 is complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# UNIT TEST: Exercise 2 - Analyze Response Quality\n",
    "# ============================================\n",
    "\n",
    "def test_analyze_response_quality():\n",
    "    \"\"\"Unit test for the analyze_response_quality method.\"\"\"\n",
    "    print(\"üß™ Testing Exercise 2: Analyze Response Quality\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create an instance of the reward model\n",
    "    test_model = GSM8KRewardSignal()\n",
    "    \n",
    "    # Test cases with expected quality indicators\n",
    "    test_cases = [\n",
    "        # Test 1: Response with calculations and steps\n",
    "        {\n",
    "            \"response\": \"First, let's add 5 + 3 = 8. Then multiply by 2 to get 16.\",\n",
    "            \"description\": \"Response with math operations and reasoning\",\n",
    "            \"expected\": {\n",
    "                \"has_calculation\": True,\n",
    "                \"has_steps\": True,\n",
    "                \"has_numbers\": True,\n",
    "                \"has_reasoning\": True,  # Changed from has_reasoning_words\n",
    "                \"min_length\": 30,\n",
    "                \"min_sentences\": 1\n",
    "            }\n",
    "        },\n",
    "\n",
    "        # Test 2: Simple answer without work\n",
    "        {\n",
    "            \"response\": \"42\",\n",
    "            \"description\": \"Single number response\",\n",
    "            \"expected\": {\n",
    "                \"has_calculation\": False,\n",
    "                \"has_steps\": False,\n",
    "                \"has_numbers\": True,\n",
    "                \"has_reasoning\": False,\n",
    "                \"max_length\": 10,\n",
    "                \"max_sentences\": 1\n",
    "            }\n",
    "        },\n",
    "\n",
    "        # Test 3: Detailed step-by-step solution\n",
    "        {\n",
    "            \"response\": \"Step 1: Calculate 10 * 5 = 50. Step 2: Subtract 15 from 50 to get 35. Finally, divide by 7 for the answer.\",\n",
    "            \"description\": \"Numbered steps with calculations\",\n",
    "            \"expected\": {\n",
    "                \"has_calculation\": True,\n",
    "                \"has_steps\": True,\n",
    "                \"has_numbers\": True,\n",
    "                \"has_reasoning\": True,\n",
    "                \"min_length\": 50,\n",
    "                \"min_sentences\": 2\n",
    "            }\n",
    "        },\n",
    "\n",
    "        # Test 4: \"I don't know\" response\n",
    "        {\n",
    "            \"response\": \"I don't know\",\n",
    "            \"description\": \"Give-up response\",\n",
    "            \"expected\": {\n",
    "                \"has_calculation\": False,\n",
    "                \"has_steps\": False,\n",
    "                \"has_numbers\": False,\n",
    "                \"has_reasoning\": False,\n",
    "                \"max_length\": 20,\n",
    "                \"max_sentences\": 1\n",
    "            }\n",
    "        },\n",
    "\n",
    "        # Test 5: Response with word-based operations\n",
    "        {\n",
    "            \"response\": \"We need to multiply twelve by three and then add seven.\",\n",
    "            \"description\": \"Word-based math operations\",\n",
    "            \"expected\": {\n",
    "                \"has_calculation\": True,\n",
    "                \"has_steps\": False,\n",
    "                \"has_numbers\": False,  # Changed: No digit characters in response\n",
    "                \"has_reasoning\": True,\n",
    "                \"min_length\": 20,\n",
    "                \"min_sentences\": 1\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    passed = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for test_case in test_cases:\n",
    "        response = test_case[\"response\"]\n",
    "        description = test_case[\"description\"]\n",
    "        expected = test_case[\"expected\"]\n",
    "        \n",
    "        try:\n",
    "            quality = test_model.analyze_response_quality(response)\n",
    "            \n",
    "            test_passed = True\n",
    "            failures = []\n",
    "            \n",
    "            # Check boolean indicators\n",
    "            for key in [\"has_calculation\", \"has_steps\", \"has_numbers\", \"has_reasoning_words\"]:\n",
    "                if key in expected:\n",
    "                    if quality.get(key) != expected[key]:\n",
    "                        failures.append(f\"{key}: expected {expected[key]}, got {quality.get(key)}\")\n",
    "                        test_passed = False\n",
    "            \n",
    "            # Check response length\n",
    "            if \"min_length\" in expected and quality.get(\"response_length\", 0) < expected[\"min_length\"]:\n",
    "                failures.append(f\"response_length: expected >= {expected['min_length']}, got {quality.get('response_length', 0)}\")\n",
    "                test_passed = False\n",
    "            if \"max_length\" in expected and quality.get(\"response_length\", 0) > expected[\"max_length\"]:\n",
    "                failures.append(f\"response_length: expected <= {expected['max_length']}, got {quality.get('response_length', 0)}\")\n",
    "                test_passed = False\n",
    "            \n",
    "            # Check sentence count\n",
    "            if \"min_sentences\" in expected and quality.get(\"sentence_count\", 0) < expected[\"min_sentences\"]:\n",
    "                failures.append(f\"sentence_count: expected >= {expected['min_sentences']}, got {quality.get('sentence_count', 0)}\")\n",
    "                test_passed = False\n",
    "            if \"max_sentences\" in expected and quality.get(\"sentence_count\", 0) > expected[\"max_sentences\"]:\n",
    "                failures.append(f\"sentence_count: expected <= {expected['max_sentences']}, got {quality.get('sentence_count', 0)}\")\n",
    "                test_passed = False\n",
    "            \n",
    "            if test_passed:\n",
    "                print(f\"‚úÖ PASSED: {description}\")\n",
    "                passed += 1\n",
    "            else:\n",
    "                print(f\"‚ùå FAILED: {description}\")\n",
    "                print(f\"   Response: '{response[:50]}...' \" if len(response) > 50 else f\"   Response: '{response}'\")\n",
    "                for failure in failures:\n",
    "                    print(f\"   - {failure}\")\n",
    "                failed += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ERROR: {description}\")\n",
    "            print(f\"   Exception: {e}\")\n",
    "            failed += 1\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Results: {passed}/{len(test_cases)} passed, {failed}/{len(test_cases)} failed\")\n",
    "    \n",
    "    if failed == 0:\n",
    "        print(\"üéâ All tests passed! Exercise 2 is complete.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Some tests failed. Please review your implementation.\")\n",
    "        print(\"\\nMinimum requirements:\")\n",
    "        print(\"- Detect math operations (symbols AND words like 'multiply', 'add')\")\n",
    "        print(\"- Identify reasoning words ('first', 'then', 'next', 'finally')\")\n",
    "        print(\"- Detect step indicators ('step', numbered lists)\")\n",
    "        print(\"- Count sentences and measure response length\")\n",
    "        print(\"- Check for presence of numbers\")\n",
    "    \n",
    "    return passed == len(test_cases)\n",
    "\n",
    "# Run the test\n",
    "test_analyze_response_quality()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Exercise 3: Reward Unparseable Responses\n",
    "\n",
    "Not all model responses will contain a clear numerical answer that you can extract. Sometimes the model might provide a detailed explanation without a final answer, give up with \"I don't know\", or produce garbled output. The current implementation harshly gives 0.0 reward to all unparseable responses, which doesn't help the model learn what aspects of its response were good or bad.\n",
    "\n",
    "Your task is to implement a nuanced reward system for unparseable responses. Use the quality indicators from Exercise 2 to provide graduated rewards. For instance, if a response is lengthy (over 200 characters) and contains calculations, it shows the model is attempting to solve the problem and deserves some credit (perhaps 0.2). If it has reasoning and numbers but no clear answer, it might deserve 0.1. Very short responses (under 20 characters) are likely dismissive responses like \"I don't know\" and should receive 0.0. \n",
    "\n",
    "The goal is to guide the model toward better responses even when it doesn't produce a parseable answer, encouraging it to show its work and reasoning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# GRADED CELL: exercise 3\n",
    "\n",
    "# This cell will be graded - compute_unparseable_reward method\n",
    "def compute_unparseable_reward(self, response: str, correct_answer: float,\n",
    "                                quality: Dict[str, any], question: str = None) -> float:\n",
    "    \"\"\"\n",
    "    Compute reward for unparseable responses (no numerical answer found).\n",
    "\n",
    "    Instead of giving 0.0 to all unparseable responses, give partial\n",
    "    credit based on the quality of the attempt.\n",
    "\n",
    "    Args:\n",
    "        response: The model's generated response\n",
    "        correct_answer: The correct numerical answer\n",
    "        quality: Quality indicators from analyze_response_quality\n",
    "\n",
    "    Returns:\n",
    "        Reward between 0.0 and 0.3\n",
    "    \"\"\"\n",
    "\n",
    "    response_length = quality.get('response_length', 0)\n",
    "    has_calculation = quality.get('has_calculation', False)\n",
    "    has_steps = quality.get('has_steps', False)\n",
    "    has_numbers = quality.get('has_numbers', False)\n",
    "\n",
    "    # Very short responses get minimal credit\n",
    "    if response_length < 20:\n",
    "        return 0.0\n",
    "\n",
    "    # Build up reward based on effort indicators\n",
    "    reward = 0.05  # Base reward for any attempt\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "\n",
    "    # If response has calculations ‚Üí reward plus 0.05\n",
    "    if has_calculation:\n",
    "        reward += 0.05\n",
    "\n",
    "    # If response has steps ‚Üí reward plus 0.05\n",
    "    if has_steps:\n",
    "        reward += 0.05\n",
    "\n",
    "    # If response has numbers ‚Üí reward plus 0.05\n",
    "    if has_numbers:\n",
    "        reward += 0.05\n",
    "\n",
    "    # If response is long (>100 chars) ‚Üí reward plus 0.05\n",
    "    if response_length > 100:\n",
    "        reward += 0.05\n",
    "\n",
    "    # If response is very long (>200 chars) ‚Üí reward plus another 0.05\n",
    "    if response_length > 200:\n",
    "        reward += 0.05\n",
    "\n",
    "    ### END CODE HERE ### \n",
    "\n",
    "    self.logger.info(f\"‚ö™ UNPARSEABLE RESPONSE:\")\n",
    "    self.logger.info(f\"  Response: {response[:200]}...\")\n",
    "    self.logger.info(f\"  Expected: {correct_answer}\")\n",
    "    self.logger.info(f\"  Reward: {reward}\")\n",
    "\n",
    "    # Cap reward at 0.3\n",
    "    return min(0.3, reward)\n",
    "\n",
    "# Add the method to GSM8KRewardSignal class for grading purposes\n",
    "GSM8KRewardSignal.compute_unparseable_reward = compute_unparseable_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Exercise 3: Compute Unparseable Reward\n",
      "==================================================\n",
      "‚úÖ PASSED: Long response with calculations (>200 chars)\n",
      "   Reward: 0.20 (expected: 0.15-0.25)\n",
      "‚úÖ PASSED: Very short response (<20 chars)\n",
      "   Reward: 0.00 (expected: 0.00-0.00)\n",
      "‚úÖ PASSED: Medium response with reasoning\n",
      "   Reward: 0.10 (expected: 0.05-0.15)\n",
      "‚úÖ PASSED: Has numbers but no clear answer\n",
      "   Reward: 0.10 (expected: 0.05-0.15)\n",
      "‚úÖ PASSED: Long detailed methodology without numbers\n",
      "   Reward: 0.20 (expected: 0.10-0.25)\n",
      "\n",
      "==================================================\n",
      "Results: 5/5 passed, 0/5 failed\n",
      "üéâ All tests passed! Exercise 3 is complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# UNIT TEST: Exercise 3 - Compute Unparseable Reward\n",
    "# ============================================\n",
    "\n",
    "def test_compute_unparseable_reward():\n",
    "    \"\"\"Unit test for the compute_unparseable_reward method.\"\"\"\n",
    "    print(\"üß™ Testing Exercise 3: Compute Unparseable Reward\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create an instance of the reward model\n",
    "    test_model = GSM8KRewardSignal()\n",
    "    \n",
    "    # Test cases: (response, quality_dict, expected_reward_range, description)\n",
    "    test_cases = [\n",
    "        # Test 1: Long response with calculations (good attempt)\n",
    "        {\n",
    "            \"response\": \"Let me work through this step by step. First I need to add the numbers together, then multiply by the factor mentioned. The calculation involves several steps that I'll show below.\",\n",
    "            \"quality\": {\n",
    "                \"has_calculation\": True,\n",
    "                \"has_steps\": True,\n",
    "                \"has_numbers\": False,\n",
    "                \"response_length\": 180,\n",
    "                \"has_reasoning_words\": True,\n",
    "                \"sentence_count\": 3\n",
    "            },\n",
    "            \"min_reward\": 0.15,\n",
    "            \"max_reward\": 0.25,\n",
    "            \"description\": \"Long response with calculations (>200 chars)\"\n",
    "        },\n",
    "        \n",
    "        # Test 2: Very short dismissive response\n",
    "        {\n",
    "            \"response\": \"I don't know\",\n",
    "            \"quality\": {\n",
    "                \"has_calculation\": False,\n",
    "                \"has_steps\": False,\n",
    "                \"has_numbers\": False,\n",
    "                \"response_length\": 12,\n",
    "                \"has_reasoning_words\": False,\n",
    "                \"sentence_count\": 1\n",
    "            },\n",
    "            \"min_reward\": 0.0,\n",
    "            \"max_reward\": 0.0,\n",
    "            \"description\": \"Very short response (<20 chars)\"\n",
    "        },\n",
    "        \n",
    "        # Test 3: Medium response with some reasoning\n",
    "        {\n",
    "            \"response\": \"First calculate the total, then find the average\",\n",
    "            \"quality\": {\n",
    "                \"has_calculation\": False,\n",
    "                \"has_steps\": True,\n",
    "                \"has_numbers\": False,\n",
    "                \"response_length\": 49,\n",
    "                \"has_reasoning_words\": True,\n",
    "                \"sentence_count\": 1\n",
    "            },\n",
    "            \"min_reward\": 0.05,\n",
    "            \"max_reward\": 0.15,\n",
    "            \"description\": \"Medium response with reasoning\"\n",
    "        },\n",
    "        \n",
    "        # Test 4: Response with numbers but no final answer\n",
    "        {\n",
    "            \"response\": \"We have 10 apples and 5 oranges to work with\",\n",
    "            \"quality\": {\n",
    "                \"has_calculation\": False,\n",
    "                \"has_steps\": False,\n",
    "                \"has_numbers\": True,\n",
    "                \"response_length\": 45,\n",
    "                \"has_reasoning_words\": False,\n",
    "                \"sentence_count\": 1\n",
    "            },\n",
    "            \"min_reward\": 0.05,\n",
    "            \"max_reward\": 0.15,\n",
    "            \"description\": \"Has numbers but no clear answer\"\n",
    "        },\n",
    "        \n",
    "        # Test 5: Long detailed attempt without parseable answer\n",
    "        {\n",
    "            \"response\": \"To solve this problem, I would first identify all the given values. Then I'd set up the equation properly. Next, I'd solve for the unknown variable. Finally, I'd check my answer to make sure it makes sense in the context.\",\n",
    "            \"quality\": {\n",
    "                \"has_calculation\": False,\n",
    "                \"has_steps\": True,\n",
    "                \"has_numbers\": False,\n",
    "                \"response_length\": 225,\n",
    "                \"has_reasoning_words\": True,\n",
    "                \"sentence_count\": 4\n",
    "            },\n",
    "            \"min_reward\": 0.1,\n",
    "            \"max_reward\": 0.25,\n",
    "            \"description\": \"Long detailed methodology without numbers\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    passed = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for test_case in test_cases:\n",
    "        response = test_case[\"response\"]\n",
    "        quality = test_case[\"quality\"]\n",
    "        min_reward = test_case[\"min_reward\"]\n",
    "        max_reward = test_case[\"max_reward\"]\n",
    "        description = test_case[\"description\"]\n",
    "        \n",
    "        try:\n",
    "            reward = test_model.compute_unparseable_reward(\n",
    "                response=response,\n",
    "                correct_answer=42.0,  # Arbitrary correct answer\n",
    "                quality=quality,\n",
    "                question=\"Test question\"\n",
    "            )\n",
    "            \n",
    "            if min_reward <= reward <= max_reward:\n",
    "                print(f\"‚úÖ PASSED: {description}\")\n",
    "                print(f\"   Reward: {reward:.2f} (expected: {min_reward:.2f}-{max_reward:.2f})\")\n",
    "                passed += 1\n",
    "            else:\n",
    "                print(f\"‚ùå FAILED: {description}\")\n",
    "                print(f\"   Response length: {quality['response_length']}\")\n",
    "                print(f\"   Reward: {reward:.2f} (expected: {min_reward:.2f}-{max_reward:.2f})\")\n",
    "                failed += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ERROR: {description}\")\n",
    "            print(f\"   Exception: {e}\")\n",
    "            failed += 1\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Results: {passed}/{len(test_cases)} passed, {failed}/{len(test_cases)} failed\")\n",
    "    \n",
    "    if failed == 0:\n",
    "        print(\"üéâ All tests passed! Exercise 3 is complete.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Some tests failed. Please review your implementation.\")\n",
    "        print(\"\\nMinimum requirements:\")\n",
    "        print(\"- Long responses (>200 chars) with calculations ‚Üí 0.2 reward\")\n",
    "        print(\"- Responses with reasoning and numbers ‚Üí 0.1 reward\")\n",
    "        print(\"- Very short responses (<20 chars) ‚Üí 0.0 reward\")\n",
    "        print(\"- Any reasonable attempt ‚Üí at least 0.05 reward\")\n",
    "    \n",
    "    return passed == len(test_cases)\n",
    "\n",
    "# Run the test\n",
    "test_compute_unparseable_reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Exercise 4: Reward High-Quality Correct Answers\n",
    "\n",
    "When the model produces the correct answer, you want to encourage not just correctness but also good mathematical communication. A response that simply states \"8\" is correct but less valuable than one that shows \"5 + 3 = 8\" with clear reasoning steps.\n",
    "\n",
    "Your task is to implement a bonus system on top of the base 1.0 reward for correct answers. Use the quality indicators to identify exemplary responses. For example, if the response shows clear step-by-step reasoning (has_steps is true), add a 0.2 bonus. If it provides a detailed explanation (response_length > 100 characters), add 0.1. If it uses proper mathematical reasoning words, add another 0.05.\n",
    "\n",
    "This bonus system teaches the model that you value not just the right answer but also the problem-solving process, which is crucial for building trust in AI systems and helping users understand the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# GRADED CELL: exercise 4\n",
    "\n",
    "def compute_correct_reward(self, response: str, predicted: float,\n",
    "                            correct_answer: float, quality: Dict[str, any], question: str = None) -> float:\n",
    "    \"\"\"\n",
    "    Compute reward for correct answers.\n",
    "\n",
    "    Base reward is 1.0, with bonuses for showing high-quality work.\n",
    "\n",
    "    Args:\n",
    "        response: The model's generated response\n",
    "        predicted: The extracted numerical answer\n",
    "        correct_answer: The correct numerical answer\n",
    "        quality: Quality indicators from analyze_response_quality\n",
    "\n",
    "    Returns:\n",
    "        Reward between 1.0 and 1.3\n",
    "    \"\"\"\n",
    "\n",
    "    reward = 1.0  # Base reward for correct answer\n",
    "\n",
    "    # Bonus for showing steps (encourages explanation)\n",
    "    if quality.get('has_steps', False):\n",
    "        reward += 0.1\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "\n",
    "    # Add +0.1 bonus for detailed explanation (response_length > 100)\n",
    "    if quality.get(\"response_length\") > 100:\n",
    "        reward += 0.1\n",
    "\n",
    "    # Add +0.05 bonus for using proper reasoning words\n",
    "    if quality.get(\"has_reasoning_words\",False):\n",
    "        reward += 0.05\n",
    "\n",
    "    # Add +0.05 bonus for showing calculations\n",
    "    if quality.get(\"has_calculation\",False):\n",
    "        reward += 0.05\n",
    "\n",
    "    # Cap the maximum reward at 1.3\n",
    "    reward = min(1.3, reward)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    self.logger.info(f\"‚úÖ CORRECT ANSWER:\")\n",
    "    self.logger.info(f\"  Predicted: {predicted}\")\n",
    "    self.logger.info(f\"  Expected: {correct_answer}\")\n",
    "    self.logger.info(f\"  Reward: {reward}\")\n",
    "    \n",
    "    return reward\n",
    "\n",
    "# Add the method to GSM8KRewardSignal class for grading purposes\n",
    "GSM8KRewardSignal.compute_correct_reward = compute_correct_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Exercise 4: Compute Correct Reward\n",
      "==================================================\n",
      "‚úÖ PASSED: Perfect answer with all quality indicators (should get bonuses)\n",
      "   Reward: 1.150 (expected: 1.150 - 1.300)\n",
      "‚úÖ PASSED: Correct but minimal response (no bonuses)\n",
      "   Reward: 1.000 (expected: 1.000 - 1.000)\n",
      "‚úÖ PASSED: Correct with calculation but no steps (partial bonus)\n",
      "   Reward: 1.050 (expected: 1.050 - 1.150)\n",
      "\n",
      "==================================================\n",
      "Results: 3/3 passed, 0/3 failed\n",
      "üéâ All tests passed! Exercise 4 is complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# UNIT TEST: Exercise 4 - Compute Correct Reward\n",
    "# ============================================\n",
    "\n",
    "def test_compute_correct_reward():\n",
    "    \"\"\"Unit test for the compute_correct_reward method.\"\"\"\n",
    "    print(\"üß™ Testing Exercise 4: Compute Correct Reward\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create an instance of the reward model\n",
    "    test_model = GSM8KRewardSignal()\n",
    "    \n",
    "    # Test cases with quality indicators and expected rewards\n",
    "    # UPDATED: More strict test cases to properly test bonus implementation\n",
    "    test_cases = [\n",
    "        # Test 1: Perfect answer with all quality indicators - should get bonuses\n",
    "        {\n",
    "            \"response\": \"Step 1: Add 5 + 3 = 8. Step 2: Multiply by 2 = 16. Therefore, the answer is 16.\",\n",
    "            \"predicted\": 16.0,\n",
    "            \"correct\": 16.0,\n",
    "            \"quality\": {\n",
    "                \"has_calculation\": True,\n",
    "                \"has_steps\": True,\n",
    "                \"has_reasoning\": True,\n",
    "                \"response_length\": 80,\n",
    "                \"sentence_count\": 3\n",
    "            },\n",
    "            \"min_reward\": 1.15,  # Raised from 1.0 - must have bonuses for quality\n",
    "            \"max_reward\": 1.3,\n",
    "            \"description\": \"Perfect answer with all quality indicators (should get bonuses)\"\n",
    "        },\n",
    "        # Test 2: Correct but minimal response - should be exactly 1.0\n",
    "        {\n",
    "            \"response\": \"16\",\n",
    "            \"predicted\": 16.0,\n",
    "            \"correct\": 16.0,\n",
    "            \"quality\": {\n",
    "                \"has_calculation\": False,\n",
    "                \"has_steps\": False,\n",
    "                \"has_reasoning\": False,\n",
    "                \"response_length\": 2,\n",
    "                \"sentence_count\": 1\n",
    "            },\n",
    "            \"min_reward\": 1.0,\n",
    "            \"max_reward\": 1.0,  # Must be exactly 1.0 - no bonuses\n",
    "            \"description\": \"Correct but minimal response (no bonuses)\"\n",
    "        },\n",
    "        # Test 3: Correct with some work but not perfect - should get partial bonuses\n",
    "        {\n",
    "            \"response\": \"The calculation is 5 + 3 = 8, then 8 * 2 = 16.\",\n",
    "            \"predicted\": 16.0,\n",
    "            \"correct\": 16.0,\n",
    "            \"quality\": {\n",
    "                \"has_calculation\": True,\n",
    "                \"has_steps\": False,\n",
    "                \"has_reasoning\": False,\n",
    "                \"response_length\": 50,\n",
    "                \"sentence_count\": 1\n",
    "            },\n",
    "            \"min_reward\": 1.05,  # Should get some bonus but not max\n",
    "            \"max_reward\": 1.15,\n",
    "            \"description\": \"Correct with calculation but no steps (partial bonus)\"\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    passed = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for test_case in test_cases:\n",
    "        response = test_case[\"response\"]\n",
    "        predicted = test_case[\"predicted\"]\n",
    "        correct = test_case[\"correct\"]\n",
    "        quality = test_case[\"quality\"]\n",
    "        min_reward = test_case[\"min_reward\"]\n",
    "        max_reward = test_case[\"max_reward\"]\n",
    "        description = test_case[\"description\"]\n",
    "        \n",
    "        try:\n",
    "            reward = test_model.compute_correct_reward(\n",
    "                response=response,\n",
    "                predicted=predicted,\n",
    "                correct_answer=correct,\n",
    "                quality=quality,\n",
    "            )\n",
    "            \n",
    "            if min_reward <= reward <= max_reward:\n",
    "                print(f\"‚úÖ PASSED: {description}\")\n",
    "                print(f\"   Reward: {reward:.3f} (expected: {min_reward:.3f} - {max_reward:.3f})\")\n",
    "                passed += 1\n",
    "            else:\n",
    "                print(f\"‚ùå FAILED: {description}\")\n",
    "                print(f\"   Reward: {reward:.3f} (expected: {min_reward:.3f} - {max_reward:.3f})\")\n",
    "                failed += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ERROR: {description}\")\n",
    "            print(f\"   Exception: {e}\")\n",
    "            failed += 1\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Results: {passed}/{len(test_cases)} passed, {failed}/{len(test_cases)} failed\")\n",
    "    \n",
    "    if failed == 0:\n",
    "        print(\"üéâ All tests passed! Exercise 4 is complete.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Some tests failed. Please review your implementation:\")\n",
    "        print(\"Expected behavior:\")\n",
    "        print(\"- Base reward: 1.0 for correct answer\")\n",
    "        print(\"- +0.1 bonus for showing steps (has_steps=True)\")\n",
    "        print(\"- +0.1 bonus for detailed explanation (>100 chars)\")\n",
    "        print(\"- +0.05 bonus for using reasoning words\")\n",
    "        print(\"- +0.05 bonus for showing calculations\")\n",
    "        print(\"- Max reward: 1.3\")\n",
    "    \n",
    "    return passed == len(test_cases)\n",
    "\n",
    "# Run the test\n",
    "test_compute_correct_reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Exercise 5: Implement Partial Credit for Wrong Answers\n",
    "\n",
    "This is perhaps the most critical exercise for effective GRPO training. The current implementation gives 0.0 reward to all wrong answers, which means the model learns nothing from near-misses or partially correct solutions. This binary reward system (1.0 for correct, 0.0 for wrong) makes learning extremely difficult and slow.\n",
    "\n",
    "Your task is to implement a sophisticated partial credit system based on how close the wrong answer is to being correct. Start by calculating the relative error between the predicted and correct answers. Responses within 1% of the correct answer should receive 0.9 reward (they're almost right!), within 10% should get 0.7, and within 30% should get 0.5. \n",
    "\n",
    "Additionally, check if the answer is at least in the right order of magnitude (between 0.1x and 10x the correct answer) and give 0.3 reward if so. Any reasonable attempt should get at least 0.1 reward. Finally, add a bonus of 0.1 if the response shows work (has calculations and steps) even though the final answer is wrong - this encourages the model to show its reasoning, making it easier to debug and improve.\n",
    "\n",
    "This graduated reward system is essential for GRPO because it provides a learning gradient - the model can learn that some wrong answers are \"less wrong\" than others and gradually improve toward the correct solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# GRADED CELL: exercise 5\n",
    "\n",
    "def compute_wrong_reward(self, response: str, predicted: float,\n",
    "                        correct_answer: float, quality: Dict[str, any], question: str = None) -> float:\n",
    "    \"\"\"\n",
    "    Compute partial credit for wrong answers.\n",
    "\n",
    "    This is critical for learning! Instead of 0.0 for all wrong answers,\n",
    "    give partial credit based on:\n",
    "    1. How close the answer is\n",
    "    2. Whether work was shown\n",
    "    3. Quality of reasoning\n",
    "\n",
    "    Args:\n",
    "        response: The model's generated response\n",
    "        predicted: The extracted numerical answer\n",
    "        correct_answer: The correct numerical answer\n",
    "        quality: Quality indicators from analyze_response_quality\n",
    "\n",
    "    Returns:\n",
    "        Reward between 0.1 and 0.9\n",
    "    \"\"\"\n",
    "\n",
    "    reward = 0.0  # Currently no partial credit - this is the main problem!\n",
    "\n",
    "    # Calculate relative error\n",
    "    if correct_answer != 0:\n",
    "        relative_error = abs(predicted - correct_answer) / abs(correct_answer)\n",
    "    else:\n",
    "        relative_error = abs(predicted - correct_answer)\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    # Base reward based on how close the answer is\n",
    "    # Within 1% error ‚Üí 0.9 reward\n",
    "    if relative_error < 0.01:\n",
    "        reward += 0.9 \n",
    "    # Within 5% error ‚Üí 0.7 reward\n",
    "    elif relative_error < 0.05:\n",
    "        reward = reward + 0.7\n",
    "    # Within 10% error ‚Üí 0.5 reward\n",
    "    elif relative_error < 0.1:\n",
    "        reward = reward + 0.5\n",
    "    # Within 30% error ‚Üí 0.3 reward\n",
    "    elif relative_error < 0.3:\n",
    "        reward = reward + 0.3\n",
    "    # Any attempt ‚Üí 0.1 reward  \n",
    "    else:\n",
    "        reward = reward + 0.1\n",
    "\n",
    "    # Add bonus (+0.1) for showing calculations and steps (has_calculation and has_steps)\n",
    "    if quality.get(\"has_calculation\",False) and quality.get(\"has_steps\",False) :\n",
    "        reward += 0.1\n",
    "    # Add bonus (+0.05) for only showing calculations words, but no steps (has_calculation)\n",
    "    elif quality.get(\"has_calculation\",False):\n",
    "        reward += 0.05\n",
    "\n",
    "    # Small bonus for longer, more detailed responses\n",
    "    if quality.get(\"response_length\") > 200:\n",
    "        reward += 0.05\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Ensure minimum reward for any attempt\n",
    "    reward = max(0.1, reward)\n",
    "\n",
    "    # Cap at 0.9 (to keep it below correct answers)\n",
    "    reward = min(0.9, reward)\n",
    "\n",
    "    self.logger.info(f\"‚ùå WRONG ANSWER:\")\n",
    "    self.logger.info(f\"  Predicted: {predicted}\")\n",
    "    self.logger.info(f\"  Expected: {correct_answer}\")\n",
    "    self.logger.info(f\"  Reward: {reward}\")\n",
    "\n",
    "    return reward\n",
    "\n",
    "# Add the method to GSM8KRewardSignal class for grading purposes\n",
    "GSM8KRewardSignal.compute_wrong_reward = compute_wrong_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Exercise 5: Compute Wrong Reward\n",
      "==================================================\n",
      "‚úÖ PASSED: Within 1% error (0.5% off)\n",
      "   Error: 0.50%, Reward: 0.90 (expected: 0.85-0.95)\n",
      "‚úÖ PASSED: Within 10% error\n",
      "   Error: 10.00%, Reward: 0.30 (expected: 0.30-0.40)\n",
      "‚úÖ PASSED: Within 30% error\n",
      "   Error: 30.00%, Reward: 0.10 (expected: 0.10-0.15)\n",
      "‚úÖ PASSED: Right order of magnitude (5x off)\n",
      "   Error: 400.00%, Reward: 0.10 (expected: 0.10-0.35)\n",
      "‚úÖ PASSED: Wrong but shows detailed work\n",
      "   Error: 50.00%, Reward: 0.20 (expected: 0.15-0.25)\n",
      "‚úÖ PASSED: Very wrong answer\n",
      "   Error: 9999900.00%, Reward: 0.10 (expected: 0.05-0.15)\n",
      "\n",
      "==================================================\n",
      "Results: 6/6 passed, 0/6 failed\n",
      "üéâ All tests passed! Exercise 5 is complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# UNIT TEST: Exercise 5 - Compute Wrong Reward\n",
    "# ============================================\n",
    "\n",
    "def test_compute_wrong_reward():\n",
    "    \"\"\"Unit test for the compute_wrong_reward method.\"\"\"\n",
    "    print(\"üß™ Testing Exercise 5: Compute Wrong Reward\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create an instance of the reward model\n",
    "    test_model = GSM8KRewardSignal()\n",
    "    \n",
    "    # Test cases with various error levels\n",
    "    test_cases = [\n",
    "        # Test 1: Almost correct (within 1%)\n",
    "        {\n",
    "            \"response\": \"The answer is 100.5\",\n",
    "            \"predicted\": 100.5,\n",
    "            \"correct\": 100.0,\n",
    "            \"quality\": {\n",
    "                \"has_calculation\": False,\n",
    "                \"has_steps\": False,\n",
    "                \"has_numbers\": True,\n",
    "                \"response_length\": 19,\n",
    "                \"has_reasoning_words\": False,\n",
    "                \"sentence_count\": 1\n",
    "            },\n",
    "            \"min_reward\": 0.85,\n",
    "            \"max_reward\": 0.95,\n",
    "            \"description\": \"Within 1% error (0.5% off)\"\n",
    "        },\n",
    "        \n",
    "        # Test 2: Close but wrong (within 10%)\n",
    "        {\n",
    "            \"response\": \"After calculations, I get 45\",\n",
    "            \"predicted\": 45.0,\n",
    "            \"correct\": 50.0,\n",
    "            \"quality\": {\n",
    "                \"has_calculation\": False,\n",
    "                \"has_steps\": False,\n",
    "                \"has_numbers\": True,\n",
    "                \"response_length\": 28,\n",
    "                \"has_reasoning_words\": False,\n",
    "                \"sentence_count\": 1\n",
    "            },\n",
    "            \"min_reward\": 0.3,\n",
    "            \"max_reward\": 0.4,\n",
    "            \"description\": \"Within 10% error\"\n",
    "        },\n",
    "        \n",
    "        # Test 3: Moderately wrong (within 30%)\n",
    "        {\n",
    "            \"response\": \"The result is 70\",\n",
    "            \"predicted\": 70.0,\n",
    "            \"correct\": 100.0,\n",
    "            \"quality\": {\n",
    "                \"has_calculation\": False,\n",
    "                \"has_steps\": False,\n",
    "                \"has_numbers\": True,\n",
    "                \"response_length\": 16,\n",
    "                \"has_reasoning_words\": False,\n",
    "                \"sentence_count\": 1\n",
    "            },\n",
    "            \"min_reward\": 0.1,  # 30% error (0.3) falls into >30% bracket (0.1 base)\n",
    "            \"max_reward\": 0.15,\n",
    "            \"description\": \"Within 30% error\"\n",
    "        },\n",
    "        \n",
    "        # Test 4: Right order of magnitude\n",
    "        {\n",
    "            \"response\": \"Approximately 500\",\n",
    "            \"predicted\": 500.0,\n",
    "            \"correct\": 100.0,\n",
    "            \"quality\": {\n",
    "                \"has_calculation\": False,\n",
    "                \"has_steps\": False,\n",
    "                \"has_numbers\": True,\n",
    "                \"response_length\": 17,\n",
    "                \"has_reasoning_words\": False,\n",
    "                \"sentence_count\": 1\n",
    "            },\n",
    "            \"min_reward\": 0.1,\n",
    "            \"max_reward\": 0.35,\n",
    "            \"description\": \"Right order of magnitude (5x off)\"\n",
    "        },\n",
    "        \n",
    "        # Test 5: Wrong but shows work\n",
    "        {\n",
    "            \"response\": \"Step 1: Add 10 + 20 = 30. Step 2: Multiply by 3 = 90. The answer is 90.\",\n",
    "            \"predicted\": 90.0,\n",
    "            \"correct\": 60.0,\n",
    "            \"quality\": {\n",
    "                \"has_calculation\": True,\n",
    "                \"has_steps\": True,\n",
    "                \"has_numbers\": True,\n",
    "                \"response_length\": 72,\n",
    "                \"has_reasoning_words\": False,\n",
    "                \"sentence_count\": 3\n",
    "            },\n",
    "            \"min_reward\": 0.15,  # 0.5 for 50% error + 0.1 for showing work\n",
    "            \"max_reward\": 0.25,\n",
    "            \"description\": \"Wrong but shows detailed work\"\n",
    "        },\n",
    "        \n",
    "        # Test 6: Very wrong\n",
    "        {\n",
    "            \"response\": \"The answer is 1000000\",\n",
    "            \"predicted\": 1000000.0,\n",
    "            \"correct\": 10.0,\n",
    "            \"quality\": {\n",
    "                \"has_calculation\": False,\n",
    "                \"has_steps\": False,\n",
    "                \"has_numbers\": True,\n",
    "                \"response_length\": 21,\n",
    "                \"has_reasoning_words\": False,\n",
    "                \"sentence_count\": 1\n",
    "            },\n",
    "            \"min_reward\": 0.05,\n",
    "            \"max_reward\": 0.15,\n",
    "            \"description\": \"Very wrong answer\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    passed = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for test_case in test_cases:\n",
    "        response = test_case[\"response\"]\n",
    "        predicted = test_case[\"predicted\"]\n",
    "        correct = test_case[\"correct\"]\n",
    "        quality = test_case[\"quality\"]\n",
    "        min_reward = test_case[\"min_reward\"]\n",
    "        max_reward = test_case[\"max_reward\"]\n",
    "        description = test_case[\"description\"]\n",
    "        \n",
    "        try:\n",
    "            reward = test_model.compute_wrong_reward(\n",
    "                response=response,\n",
    "                predicted=predicted,\n",
    "                correct_answer=correct,\n",
    "                quality=quality\n",
    "            )\n",
    "            \n",
    "            # Calculate actual error for debugging\n",
    "            relative_error = abs(predicted - correct) / (abs(correct) + 1e-10)\n",
    "            \n",
    "            if min_reward <= reward <= max_reward:\n",
    "                print(f\"‚úÖ PASSED: {description}\")\n",
    "                print(f\"   Error: {relative_error:.2%}, Reward: {reward:.2f} (expected: {min_reward:.2f}-{max_reward:.2f})\")\n",
    "                passed += 1\n",
    "            else:\n",
    "                print(f\"‚ùå FAILED: {description}\")\n",
    "                print(f\"   Predicted: {predicted}, Correct: {correct}, Error: {relative_error:.2%}\")\n",
    "                print(f\"   Reward: {reward:.2f} (expected: {min_reward:.2f}-{max_reward:.2f})\")\n",
    "                failed += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ERROR: {description}\")\n",
    "            print(f\"   Exception: {e}\")\n",
    "            failed += 1\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Results: {passed}/{len(test_cases)} passed, {failed}/{len(test_cases)} failed\")\n",
    "    \n",
    "    if failed == 0:\n",
    "        print(\"üéâ All tests passed! Exercise 5 is complete.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Some tests failed. Please review your implementation.\")\n",
    "        print(\"\\nMinimum requirements (partial credit system):\")\n",
    "        print(\"- Within 1% error ‚Üí 0.9 reward\")\n",
    "        print(\"- Within 10% error ‚Üí 0.7 reward\")\n",
    "        print(\"- Within 30% error ‚Üí 0.5 reward\")\n",
    "        print(\"- Right order of magnitude (0.1x-10x) ‚Üí 0.3 reward\")\n",
    "        print(\"- Any reasonable attempt ‚Üí 0.1 reward\")\n",
    "        print(\"- +0.1 bonus for showing work (calculations + steps)\")\n",
    "    \n",
    "    return passed == len(test_cases)\n",
    "\n",
    "# Run the test\n",
    "test_compute_wrong_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reward model class defined!\n",
      "This will be your 'grading system' for model responses.\n"
     ]
    }
   ],
   "source": [
    "# This cell will be graded - compute_reward method (main orchestrator)\n",
    "def compute_reward(self, response: str, correct_answer: float, question: str = None) -> float:\n",
    "    \"\"\"\n",
    "    Main reward computation function - delegates to specialized methods.\n",
    "    \n",
    "    This function orchestrates the reward computation by:\n",
    "    1. Extracting numerical answer from response\n",
    "    2. Analyzing response quality indicators  \n",
    "    3. Calling appropriate reward computation method\n",
    "    4. Returning final reward value\n",
    "    \n",
    "    Args:\n",
    "        response: The model's response text\n",
    "        correct_answer: The correct numerical answer\n",
    "        question: The original question (optional)\n",
    "        \n",
    "    Returns:\n",
    "        float: Final reward value for this response\n",
    "    \"\"\"\n",
    "    # Step 1: Try to extract numerical answer\n",
    "    predicted = self.extract_numerical_answer(response)\n",
    "    \n",
    "    # Step 2: Analyze response quality\n",
    "    quality = self.analyze_response_quality(response)\n",
    "    \n",
    "    # Step 3: Route to appropriate reward computation\n",
    "    if predicted is None:\n",
    "        # Case 1: Could not parse any numerical answer\n",
    "        return self.compute_unparseable_reward(response, correct_answer, quality, question)\n",
    "    elif abs(predicted - correct_answer) < 0.01:\n",
    "        # Case 2: Answer is correct (within small tolerance)\n",
    "        return self.compute_correct_reward(response, predicted, correct_answer, quality, question)\n",
    "    else:\n",
    "        # Case 3: Answer is wrong\n",
    "        return self.compute_wrong_reward(response, predicted, correct_answer, quality, question)\n",
    "\n",
    "# Add the method to GSM8KRewardSignal class for grading purposes\n",
    "GSM8KRewardSignal.compute_reward = compute_reward\n",
    "\n",
    "print(\"‚úÖ Reward model class defined!\")\n",
    "print(\"This will be your 'grading system' for model responses.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ TESTING THE REWARD MODEL\n",
      "============================================================\n",
      "See how it grades different types of responses:\n",
      "\n",
      "‚úÖ Reward model created with detailed logging for debugging!\n",
      "Check the log file to see question, response, and reward details for each generation.\n",
      "Test 1:\n",
      "  Response: \"Let me calculate step by step: 5 + 3 = 8. The answ...\"\n",
      "  Extracted answer: 8.0\n",
      "  Correct answer: 8.0\n",
      "  Reward: 1.15\n",
      "  ‚úÖ Excellent!\n",
      "\n",
      "Test 2:\n",
      "  Response: \"The answer is 8\"\n",
      "  Extracted answer: 8.0\n",
      "  Correct answer: 8.0\n",
      "  Reward: 1.00\n",
      "  ‚úÖ Excellent!\n",
      "\n",
      "Test 3:\n",
      "  Response: \"5 + 3 = 7. So the answer is 7.\"\n",
      "  Extracted answer: 7.0\n",
      "  Correct answer: 8.0\n",
      "  Reward: 0.40\n",
      "  üü† Some credit\n",
      "\n",
      "Test 4:\n",
      "  Response: \"The total is 100\"\n",
      "  Extracted answer: 100.0\n",
      "  Correct answer: 8.0\n",
      "  Reward: 0.10\n",
      "  ‚ö™ Minimal credit\n",
      "\n",
      "Test 5:\n",
      "  Response: \"First I add 5 + 3 = 9. Then I multiply by 2 = 18. ...\"\n",
      "  Extracted answer: 18.0\n",
      "  Correct answer: 8.0\n",
      "  Reward: 0.20\n",
      "  üü† Some credit\n",
      "\n",
      "Test 6:\n",
      "  Response: \"I don't know how to solve this\"\n",
      "  Extracted answer: None\n",
      "  Correct answer: 8.0\n",
      "  Reward: 0.05\n",
      "  ‚ö™ Minimal credit\n",
      "\n",
      "\n",
      "üí° Note: Detailed logs are saved to: ./grpo_logs/grpo_20251105_052057.log\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Test the Reward Model\n",
    "# ============================================\n",
    "\n",
    "# Test your reward model with different types of answers\n",
    "print(\"üß™ TESTING THE REWARD MODEL\")\n",
    "print(\"=\"*60)\n",
    "print(\"See how it grades different types of responses:\\n\")\n",
    "\n",
    "reward_model = GSM8KRewardSignal()\n",
    "print(\"‚úÖ Reward model created with detailed logging for debugging!\")\n",
    "print(\"Check the log file to see question, response, and reward details for each generation.\")\n",
    "\n",
    "# Test cases: (response, correct_answer)\n",
    "test_cases = [\n",
    "    # Perfect answer with steps\n",
    "    (\"Let me calculate step by step: 5 + 3 = 8. The answer is #### 8\", 8.0),\n",
    "    \n",
    "    # Correct but no steps\n",
    "    (\"The answer is 8\", 8.0),\n",
    "    \n",
    "    # Close but wrong\n",
    "    (\"5 + 3 = 7. So the answer is 7.\", 8.0),\n",
    "    \n",
    "    # Very wrong\n",
    "    (\"The total is 100\", 8.0),\n",
    "    \n",
    "    # Shows work but wrong\n",
    "    (\"First I add 5 + 3 = 9. Then I multiply by 2 = 18. Answer: 18\", 8.0),\n",
    "    \n",
    "    # No attempt\n",
    "    (\"I don't know how to solve this\", 8.0),\n",
    "]\n",
    "\n",
    "for i, (response, correct) in enumerate(test_cases, 1):\n",
    "    print(f\"Test {i}:\")\n",
    "    print(f\"  Response: \\\"{response[:50]}...\\\"\" if len(response) > 50 else f\"  Response: \\\"{response}\\\"\")\n",
    "    \n",
    "    # Compute reward\n",
    "    reward = reward_model.compute_reward(response, correct, \"Test question\")\n",
    "    \n",
    "    # Extract answer for display\n",
    "    extracted = reward_model.extract_numerical_answer(response)\n",
    "    \n",
    "    print(f\"  Extracted answer: {extracted}\")\n",
    "    print(f\"  Correct answer: {correct}\")\n",
    "    print(f\"  Reward: {reward:.2f}\")\n",
    "    \n",
    "    # Explain the reward\n",
    "    if reward >= 1.0:\n",
    "        print(\"  ‚úÖ Excellent!\")\n",
    "    elif reward >= 0.5:\n",
    "        print(\"  üü® Good attempt\")\n",
    "    elif reward >= 0.2:\n",
    "        print(\"  üü† Some credit\")\n",
    "    else:\n",
    "        print(\"  ‚ö™ Minimal credit\")\n",
    "    print()\n",
    "\n",
    "print(f\"\\nüí° Note: Detailed logs are saved to: {log_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Load the Language Model <a id=\"loadthelanguagemodel\"></a>\n",
    "\n",
    "### About DeepSeek Math Model\n",
    "\n",
    "You will be using **DeepSeek Math 7B Base** Model\n",
    "\n",
    "### What You are Loading\n",
    "\n",
    "1. **Tokenizer**: Converts text to numbers the model understands\n",
    "2. **Model**: The actual neural network with all the parameters\n",
    "\n",
    "### Memory Requirements\n",
    "\n",
    "- The model needs about 14-28 GB of GPU memory for loading the model\n",
    "- Loading may take 1-2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "The tokenizer converts text to tokens (numbers) that the model understands.\n",
      "\n",
      "‚úÖ Tokenizer loaded successfully!\n",
      "  Vocabulary size: 100,002 tokens\n",
      "  Padding token: <ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n",
      "  End-of-sequence token: <ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n",
      "\n",
      "Example tokenization:\n",
      "  Text: \"What is 5 + 3?\"\n",
      "  Tokens: [100000, 2640, 317, 207, 20, 919, 207, 18, 30]\n",
      "  Number of tokens: 9\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Load the Tokenizer\n",
    "# ============================================\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "print(\"The tokenizer converts text to tokens (numbers) that the model understands.\\n\")\n",
    "\n",
    "is_local = os.path.exists(config.model_name)\n",
    "\n",
    "# Load the tokenizer\n",
    "# trust_remote_code=True allows loading custom code from the model repository\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.model_name,\n",
    "    trust_remote_code=True,  # Some models have custom tokenizer code\n",
    "    local_files_only=is_local\n",
    ")\n",
    "\n",
    "# Set up padding token\n",
    "# Padding is used to make all inputs the same length\n",
    "if tokenizer.pad_token is None:\n",
    "    # If no padding token defined, use the end-of-sequence token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set padding to left side for generation tasks\n",
    "# This ensures the actual text is on the right (where model expects it)\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "print(\"‚úÖ Tokenizer loaded successfully!\")\n",
    "print(f\"  Vocabulary size: {len(tokenizer):,} tokens\")\n",
    "print(f\"  Padding token: {tokenizer.pad_token}\")\n",
    "print(f\"  End-of-sequence token: {tokenizer.eos_token}\")\n",
    "\n",
    "# Example of tokenization\n",
    "example_text = \"What is 5 + 3?\"\n",
    "tokens = tokenizer.encode(example_text)\n",
    "print(f\"\\nExample tokenization:\")\n",
    "print(f\"  Text: \\\"{example_text}\\\"\")\n",
    "print(f\"  Tokens: {tokens[:10]}...\" if len(tokens) > 10 else f\"  Tokens: {tokens}\")\n",
    "print(f\"  Number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the DeepSeek Math model...\n",
      "This may take 1-2 minutes depending on your internet speed.\n",
      "\n",
      "‚úÖ Model loaded successfully!\n",
      "\n",
      "Model Information:\n",
      "  Model type: LlamaForCausalLM\n",
      "  Number of parameters: 6.91 billion\n",
      "  Location: GPU (fast training!)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Load the Language Model\n",
    "# ============================================\n",
    "\n",
    "print(\"Loading the DeepSeek Math model...\")\n",
    "print(\"This may take 1-2 minutes depending on your internet speed.\\n\")\n",
    "\n",
    "# Set up model loading arguments\n",
    "model_kwargs = {\n",
    "    \"trust_remote_code\": True,  # Allow custom model code\n",
    "    \"dtype\": torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    \"use_cache\": True,\n",
    "    \"local_files_only\": is_local,\n",
    "    # bfloat16: Uses less memory than float32 but maintains good precision\n",
    "    # float32: Full precision (used on CPU)\n",
    "}\n",
    "\n",
    "# Optional: Use 8-bit quantization to save memory\n",
    "if config.use_8bit:\n",
    "    print(\"Using 8-bit mode to save memory...\")\n",
    "    model_kwargs[\"load_in_8bit\"] = True\n",
    "    # 8-bit mode reduces memory usage by ~50% with minimal accuracy loss\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    **model_kwargs\n",
    ")\n",
    "model.to(device)\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "\n",
    "# Display model information\n",
    "print(f\"\\nModel Information:\")\n",
    "print(f\"  Model type: {model.__class__.__name__}\")\n",
    "print(f\"  Number of parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.2f} billion\")\n",
    "\n",
    "# Check model device\n",
    "if next(model.parameters()).is_cuda:\n",
    "    print(f\"  Location: GPU (fast training!)\")\n",
    "else:\n",
    "    print(f\"  Location: CPU (slower training)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling memory optimizations...\n",
      "\n",
      "‚úÖ Gradient checkpointing enabled\n",
      "   This saves memory by recomputing values when needed\n",
      "   Training will be slightly slower but use less memory\n",
      "\n",
      "Testing model with a simple math problem...\n",
      "  Prompt: \"What is 2 + 2? The answer is\"\n",
      "  Model response: \"What is 2 + 2? The answer is 4. What is 2 + 2\"\n",
      "\n",
      "‚úÖ Model is working correctly!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Enable Memory Optimizations\n",
    "# ============================================\n",
    "\n",
    "print(\"Enabling memory optimizations...\\n\")\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "# This trades computation for memory by not storing all intermediate values\n",
    "if hasattr(model, 'gradient_checkpointing_enable'):\n",
    "    model.gradient_checkpointing_enable()\n",
    "    print(\"‚úÖ Gradient checkpointing enabled\")\n",
    "    print(\"   This saves memory by recomputing values when needed\")\n",
    "    print(\"   Training will be slightly slower but use less memory\")\n",
    "else:\n",
    "    print(\"Gradient checkpointing not available for this model\")\n",
    "\n",
    "# Test the model with a simple generation\n",
    "print(\"\\nTesting model with a simple math problem...\")\n",
    "test_prompt = \"What is 2 + 2? The answer is\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Move inputs to same device as model\n",
    "if torch.cuda.is_available():\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "# Generate a short response\n",
    "with torch.no_grad():  # Don't calculate gradients for this test\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=10,\n",
    "        temperature=0.1,  # Low temperature for deterministic output\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id  # Explicitly set to suppress warning\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"  Prompt: \\\"{test_prompt}\\\"\")\n",
    "print(f\"  Model response: \\\"{response}\\\"\")\n",
    "print(\"\\n‚úÖ Model is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Prepare Training and Validation Datasets <a id=\"preparetraining\"></a>\n",
    "\n",
    "### Data Splitting\n",
    "\n",
    "You need to split the data into two parts:\n",
    "1. **Training set** (80%): Used to train the model\n",
    "2. **Validation set** (20%): Used to check progress during training\n",
    "\n",
    "### Data Preparation Steps\n",
    "\n",
    "For each problem, we:\n",
    "1. Create a prompt (the question)\n",
    "2. Extract the numerical answer\n",
    "3. Format it for the model\n",
    "\n",
    "### Prompt Template\n",
    "\n",
    "You use a specific format to help the model understand what you want:\n",
    "```\n",
    "Question: [math problem]\n",
    "Let's solve this step-by-step and find the numerical answer:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing datasets for training...\n",
      "\n",
      "\n",
      "Dataset Statistics:\n",
      "  Total examples: 7,473\n",
      "  Training examples: 5,978 (80.0%)\n",
      "  Validation examples: 1,495 (20.0%)\n",
      "\n",
      "Training Iterations:\n",
      "  Steps per epoch: 93\n",
      "  Total training steps: 465\n",
      "  Evaluations during training: 23\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Prepare Training and Validation Datasets\n",
    "# ============================================\n",
    "\n",
    "print(\"Preparing datasets for training...\\n\")\n",
    "\n",
    "# Use the utility function to prepare the datasets\n",
    "# This function:\n",
    "# 1. Loads the GSM8K training data\n",
    "# 2. Splits it into train/validation\n",
    "# 3. Formats each problem with the prompt template\n",
    "# 4. Extracts numerical answers\n",
    "train_dataset, eval_dataset = prepare_dataset(config, tokenizer)\n",
    "\n",
    "# Display dataset statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"  Total examples: {len(train_dataset) + len(eval_dataset):,}\")\n",
    "print(f\"  Training examples: {len(train_dataset):,} ({len(train_dataset)/(len(train_dataset)+len(eval_dataset))*100:.1f}%)\")\n",
    "print(f\"  Validation examples: {len(eval_dataset):,} ({len(eval_dataset)/(len(train_dataset)+len(eval_dataset))*100:.1f}%)\")\n",
    "\n",
    "# Calculate training iterations\n",
    "steps_per_epoch = len(train_dataset) // (config.per_device_train_batch_size * config.gradient_accumulation_steps)\n",
    "total_steps = steps_per_epoch * config.num_train_epochs\n",
    "print(f\"\\nTraining Iterations:\")\n",
    "print(f\"  Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"  Total training steps: {total_steps}\")\n",
    "print(f\"  Evaluations during training: {total_steps // config.eval_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Sample Training Examples:\n",
      "============================================================\n",
      "\n",
      "Example 1:\n",
      "----------------------------------------\n",
      "PROMPT (Input to model):\n",
      "Question: Stefan goes to a restaurant to eat dinner with his family. They order an appetizer that costs $10 and 4 entrees that are $20 each. If they tip 20% of the total for the waiter, what is the total amount of money that they spend at the restaurant?\n",
      "\n",
      "Let's solve this step-by-step and find the n...\n",
      "\n",
      "EXPECTED NUMERICAL ANSWER: 108.0\n",
      "\n",
      "SOLUTION STEPS (first part):\n",
      "The total cost of the entrees is 4 * $20 = $<<4*20=80>>80.\n",
      "The total cost of the dinner is $80 + $10 = $<<80+10=90>>90.\n",
      "The tip is $90 * 0.20 = $<<90*0.20=18>>18\n",
      "The total cost with tip is $90 + $18 =...\n",
      "\n",
      "Example 2:\n",
      "----------------------------------------\n",
      "PROMPT (Input to model):\n",
      "Question: The gauge on a water tank shows that the tank is 1/3 full of water. To fill the tank, 16 gallons of water are added. How many gallons of water does the tank hold when full?\n",
      "\n",
      "Let's solve this step-by-step and find the numerical answer:\n",
      "\n",
      "\n",
      "EXPECTED NUMERICAL ANSWER: 24.0\n",
      "\n",
      "SOLUTION STEPS (first part):\n",
      "Given that the tank is 1/3 full of water, and that it requires 16 gallons to fill. 1 full tank -1/3 tank = 2/3 of the tank is empty, which is equal to 16 gallons.\n",
      "If 2/3 of the tank equals 16 gallons,...\n",
      "\n",
      "Example 3:\n",
      "----------------------------------------\n",
      "PROMPT (Input to model):\n",
      "Question: Ben has 8 apples more than Phillip does. Tom has three eighths as many apples at Ben has. If Phillip has 40 apples, how many apples does Tom have?\n",
      "\n",
      "Let's solve this step-by-step and find the numerical answer:\n",
      "\n",
      "\n",
      "EXPECTED NUMERICAL ANSWER: 18.0\n",
      "\n",
      "SOLUTION STEPS (first part):\n",
      "Ben has 40+8 = <<40+8=48>>48 apples.\n",
      "There are 48/8 = <<48/8=6>>6 apples in every eighth.\n",
      "Tom has 6*3 = <<6*3=18>>18 apples.\n",
      "#### 18\n",
      "\n",
      "============================================================\n",
      "The model will learn to generate step-by-step solutions like these!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Show Sample Training Examples\n",
    "# ============================================\n",
    "\n",
    "print(\"üìù Sample Training Examples:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show 3 examples from the training set\n",
    "for i in range(min(3, len(train_dataset))):\n",
    "    sample = train_dataset[i]\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Show the prompt (that you give the model)\n",
    "    print(\"PROMPT (Input to model):\")\n",
    "    print(sample['prompt'][:300] + \"...\" if len(sample['prompt']) > 300 else sample['prompt'])\n",
    "    \n",
    "    # Show the expected answer\n",
    "    print(f\"\\nEXPECTED NUMERICAL ANSWER: {sample['answer']}\")\n",
    "    \n",
    "    # Show part of the solution\n",
    "    if 'answer_text' in sample:\n",
    "        print(\"\\nSOLUTION STEPS (first part):\")\n",
    "        print(sample['answer_text'][:200] + \"...\" if len(sample['answer_text']) > 200 else sample['answer_text'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"The model will learn to generate step-by-step solutions like these!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Create Evaluation Callback <a id=\"createevaluation\"></a>\n",
    "\n",
    "### Why Evaluation Matters\n",
    "\n",
    "During training, you want to know:\n",
    "- Is the model getting better?\n",
    "- What's the current accuracy?\n",
    "- Should you stop training?\n",
    "\n",
    "### What You Track\n",
    "\n",
    "Our evaluation callback measures:\n",
    "1. **Accuracy**: Percentage of correct answers\n",
    "2. **Average Reward**: How good the answers are overall\n",
    "3. **Sample Outputs**: Actual model responses\n",
    "\n",
    "### Evaluation Frequency\n",
    "\n",
    "You evaluate:\n",
    "- Every 20 training steps (configurable)\n",
    "- On the test set (never seen during training)\n",
    "- Using a sample for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up evaluation system...\n",
      "\n",
      "Loading GSM8K test dataset...\n",
      "‚úÖ Loaded 1,319 test examples\n",
      "   These are NEVER seen during training (prevents cheating)\n",
      "\n",
      "‚úÖ Evaluation system configured!\n",
      "\n",
      "Evaluation Details:\n",
      "  Will evaluate on: 1319 test examples\n",
      "  Evaluation frequency: Every 20 training steps\n",
      "  Metrics tracked: Accuracy, Average Reward, Sample Outputs\n",
      "\n",
      "Tip: Watch the accuracy increase during training!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Set Up Evaluation System\n",
    "# ============================================\n",
    "\n",
    "print(\"Setting up evaluation system...\\n\")\n",
    "\n",
    "# Load the test dataset for evaluation\n",
    "# IMPORTANT: you should use the TEST set (not training data) to measure true performance\n",
    "print(\"Loading GSM8K test dataset...\")\n",
    "gsm8k = load_from_disk(\"/app/data/gsm8k\")\n",
    "test_dataset = gsm8k[\"test\"]\n",
    "print(f\"‚úÖ Loaded {len(test_dataset):,} test examples\")\n",
    "print(\"   These are NEVER seen during training (prevents cheating)\\n\")\n",
    "\n",
    "# Create the evaluation callback\n",
    "# This will run periodically during training to check progress\n",
    "eval_callback = GSM8KEvaluationCallback(\n",
    "    tokenizer=tokenizer,\n",
    "    test_dataset=test_dataset,\n",
    "    batch_size=64,  # How many examples to evaluate at once\n",
    "    sample_size=1.0  # Use full test set (1.0 = 100%)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Evaluation system configured!\")\n",
    "print(f\"\\nEvaluation Details:\")\n",
    "print(f\"  Will evaluate on: {len(test_dataset)} test examples\")\n",
    "print(f\"  Evaluation frequency: Every {config.eval_steps} training steps\")\n",
    "print(f\"  Metrics tracked: Accuracy, Average Reward, Sample Outputs\")\n",
    "print(f\"\\nTip: Watch the accuracy increase during training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Configure GRPO Trainer <a id=\"configuregrpotrainer\"></a>\n",
    "\n",
    "### The GRPO Training Process\n",
    "\n",
    "Here's how GRPO training works:\n",
    "\n",
    "1. **Generate Multiple Answers**: For each question, generate 12 different answers\n",
    "2. **Score Each Answer**: Use your reward model to grade each one\n",
    "3. **Compare Within Group**: See which answers are better than others\n",
    "4. **Update Model**: Teach it to prefer the better answers\n",
    "\n",
    "Instead of saying \"this answer is worth 0.7 points\", GRPO says \"Answer #3 is better than answers #1, #2, #4, #5...\". This relative comparison is more stable and effective than scoring each answer.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Trainer**: Orchestrates the training process\n",
    "2. **Reward Function**: Grades the generated answers\n",
    "3. **Configuration**: All your settings from earlier\n",
    "\n",
    "Note the `GRPOConfig()` within the `create_grpo_trainer()` function. This is where you set various parameters such as the temperature, as you have seen in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Creating GRPO trainer...\n",
      "\n",
      "Built answer lookup with 5,978 entries\n",
      "‚úÖ GRPO configuration created\n",
      "\n",
      "Configuration ready for training!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Define the GRPO Trainer Creation Function\n",
    "# ============================================\n",
    "\n",
    "def create_grpo_trainer(config, model, tokenizer, train_dataset, eval_dataset, reward_model, test_dataset=None):\n",
    "    \"\"\"\n",
    "    Create and configure the GRPO trainer.\n",
    "    \n",
    "    This function sets up everything needed for GRPO training:\n",
    "    1. Configuration for the training process\n",
    "    2. Reward computation function\n",
    "    3. The trainer itself\n",
    "    \n",
    "    Args:\n",
    "        config: Training configuration\n",
    "        model: The language model to train\n",
    "        tokenizer: Tokenizer for the model\n",
    "        train_dataset: Training data\n",
    "        eval_dataset: Validation data\n",
    "        reward_model: Your reward scoring system\n",
    "        test_dataset: Test data for evaluation\n",
    "    \n",
    "    Returns:\n",
    "        A configured GRPO trainer ready to train\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üîß Creating GRPO trainer...\\n\")\n",
    "    \n",
    "    # Build a dictionary for fast answer lookup\n",
    "    # This maps each prompt to its correct answer\n",
    "    # O(1) lookup is much faster than searching through the dataset\n",
    "    prompt2ans = {}\n",
    "    for item in train_dataset:\n",
    "        prompt2ans[item['prompt']] = (item['answer'], item['question'])\n",
    "    print(f\"Built answer lookup with {len(prompt2ans):,} entries\")\n",
    "    \n",
    "    # Configure GRPO training parameters \n",
    "    # In most cases, you don't need to modify the parameters below\n",
    "    GRPO_config = GRPOConfig(\n",
    "        # ===== GRPO SPECIFIC =====\n",
    "        num_generations=config.num_generations,  # Generate 12 answers per question\n",
    "        temperature=config.temperature,  # Randomness in generation\n",
    "        generation_kwargs={\n",
    "            \"max_new_tokens\": config.max_new_tokens,\n",
    "            \"temperature\": config.temperature,\n",
    "            \"top_p\": 0.95,  # Only consider top 95% probability tokens\n",
    "            \"do_sample\": True,  # Enable random sampling\n",
    "            \"pad_token_id\": tokenizer.pad_token_id,\n",
    "            \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        },\n",
    "        generation_batch_size=min(96, config.per_device_train_batch_size * config.num_generations),\n",
    "        \n",
    "        # ===== OUTPUT SETTINGS =====\n",
    "        output_dir=config.output_dir,\n",
    "        \n",
    "        # ===== TRAINING DURATION =====\n",
    "        num_train_epochs=config.num_train_epochs,\n",
    "        \n",
    "        # ===== BATCH SETTINGS =====\n",
    "        per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        \n",
    "        # ===== LEARNING SETTINGS =====\n",
    "        learning_rate=config.learning_rate,\n",
    "        lr_scheduler_type=\"cosine\",  # Gradually reduce learning rate\n",
    "        warmup_ratio=0.1,  # Start with lower learning rate for stability\n",
    "        \n",
    "        # ===== OPTIMIZATION =====\n",
    "        max_grad_norm=1.0,  # Clip gradients to prevent explosions\n",
    "        adam_beta1=0.9,  # Momentum parameter\n",
    "        adam_beta2=0.95,  # Better for RL than default 0.999\n",
    "        weight_decay=0.1,  # L2 regularization to prevent overfitting\n",
    "        \n",
    "        # ===== MONITORING =====\n",
    "        logging_steps=config.logging_steps,\n",
    "        eval_steps=config.eval_steps,\n",
    "        save_steps=config.save_steps,\n",
    "        save_total_limit=config.save_total_limit,\n",
    "        \n",
    "        # ===== TECHNICAL SETTINGS =====\n",
    "        seed=config.seed,\n",
    "        bf16=True if torch.cuda.is_available() else False,  # Use bfloat16 on GPU\n",
    "        fp16=False,  # Don't use float16 (less stable)\n",
    "        remove_unused_columns=False,\n",
    "        push_to_hub=False,  # Don't upload to Hugging Face\n",
    "        report_to=[],  # Disable external logging\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ GRPO configuration created\")\n",
    "    return GRPO_config, prompt2ans\n",
    "\n",
    "# Create the configuration\n",
    "GRPO_config, prompt2ans = create_grpo_trainer(config, model, tokenizer, train_dataset, eval_dataset, reward_model, test_dataset)\n",
    "print(\"\\nConfiguration ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Reward compuation function defined with detailed logging\n",
    "\n",
    "This function will:\n",
    "  1. Score each generated answer with detailed logs\n",
    "  2. Show question, response, and reward for each generation\n",
    "  3. Compare answers within groups\n",
    "  4. Return relative rewards for training\n",
    "  5. Log group statistics for debugging\n",
    "\n",
    "In this function you can find the implementation of both the `mean_reward` and `normalized_rewards`, which you saw in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Define the Reward Computation Function\n",
    "# ============================================\n",
    "\n",
    "def compute_rewards(prompts: List[str], completions: List[str], **kwargs):\n",
    "    \"\"\"\n",
    "    Compute rewards for generated completions with detailed logging.\n",
    "    \n",
    "    This is the heart of GRPO - it scores each generated answer\n",
    "    and normalizes within groups for relative comparison.\n",
    "    \n",
    "    Process:\n",
    "    1. Group completions by their prompt\n",
    "    2. Score each completion with detailed logging\n",
    "    3. Normalize scores within each group\n",
    "    4. Return normalized rewards\n",
    "    \n",
    "    Args:\n",
    "        prompts: List of prompts (with duplicates for each generation)\n",
    "        completions: List of generated responses\n",
    "    \n",
    "    Returns:\n",
    "        List of normalized rewards\n",
    "    \"\"\"\n",
    "    \n",
    "    rewards = []\n",
    "    \n",
    "    # Log batch information\n",
    "    logger.info(f\"\\n{'='*80}\")\n",
    "    logger.info(f\"GRPO REWARD COMPUTATION\")\n",
    "    logger.info(f\"Total completions: {len(completions)}\")\n",
    "    logger.info(f\"Total prompts: {len(prompts)}\")\n",
    "    logger.info(f\"Expected generations per prompt: {config.num_generations}\")\n",
    "    logger.info(f\"{'='*80}\")\n",
    "    \n",
    "    # Get unique prompts (remove duplicates)\n",
    "    seen = set()\n",
    "    unique_prompts = []\n",
    "    for p in prompts:\n",
    "        if p not in seen:\n",
    "            unique_prompts.append(p)\n",
    "            seen.add(p)\n",
    "    \n",
    "    num_unique_prompts = len(unique_prompts)\n",
    "    logger.info(f\"Calculated unique prompts: {num_unique_prompts}\")\n",
    "    \n",
    "    # Debug: Print samples\n",
    "    for i in range(min(4, len(completions))):\n",
    "        logger.info(f\"Completion {i+1}: {completions[i][:80]}...\")\n",
    "    \n",
    "    # Process each unique prompt and its completions\n",
    "    for i, unique_prompt in enumerate(unique_prompts):\n",
    "        # Find all completions for this prompt\n",
    "        prompt_indices = [idx for idx, p in enumerate(prompts) if p == unique_prompt]\n",
    "        group_completions = [completions[idx] for idx in prompt_indices]\n",
    "        \n",
    "        # Get correct answer from your lookup dictionary\n",
    "        correct_answer, question = prompt2ans.get(unique_prompt, (0.0, \"Unknown question\"))\n",
    "        \n",
    "        logger.info(f\"\\n--- Unique Prompt {i+1}/{num_unique_prompts} ---\")\n",
    "        logger.info(f\"Question: {question[:150]}...\")\n",
    "        logger.info(f\"Expected Answer: {correct_answer}\")\n",
    "        logger.info(f\"Number of generations for this prompt: {len(group_completions)}\")\n",
    "        \n",
    "        # Compute reward for each completion with detailed logging\n",
    "        group_rewards = []\n",
    "        for j, completion in enumerate(group_completions):\n",
    "            logger.info(f\"\\n  Generation {j+1}/{len(group_completions)}:\")\n",
    "            reward = reward_model.compute_reward(completion, correct_answer, question)\n",
    "            group_rewards.append(reward)\n",
    "        \n",
    "        # Normalize rewards within the group (mean-centering)\n",
    "        # This makes rewards relative: positive = better than average, negative = worse\n",
    "        if len(group_rewards) > 1:\n",
    "            mean_reward = sum(group_rewards) / len(group_rewards)\n",
    "            # Mean-center only - no standard deviation scaling\n",
    "            normalized_rewards = [r - mean_reward for r in group_rewards]\n",
    "        else:\n",
    "            # Single generation - no comparison possible\n",
    "            normalized_rewards = [0.0]\n",
    "        \n",
    "        # Add normalized rewards in correct order\n",
    "        for idx, norm_reward in zip(prompt_indices, normalized_rewards):\n",
    "            if len(rewards) <= idx:\n",
    "                rewards.extend([0] * (idx - len(rewards) + 1))\n",
    "            rewards[idx] = norm_reward\n",
    "        \n",
    "        # Log group statistics matching your format\n",
    "        avg_reward = sum(group_rewards) / len(group_rewards)\n",
    "        max_reward = max(group_rewards)\n",
    "        min_reward = min(group_rewards)\n",
    "        avg_norm = sum(normalized_rewards) / len(normalized_rewards)\n",
    "        logger.info(f\"\\n  Group Summary: Avg={avg_reward:.3f}, Max={max_reward:.3f}, Min={min_reward:.3f}\")\n",
    "        logger.info(f\"  Normalized: Avg={avg_norm:.3f}, Rewards={[f'{r:.3f}' for r in normalized_rewards]}\")\n",
    "    \n",
    "    overall_avg = sum(rewards) / len(rewards) if rewards else 0\n",
    "    logger.info(f\"\\n{'='*80}\")\n",
    "    logger.info(f\"BATCH SUMMARY: Overall Average Reward = {overall_avg:.3f}\")\n",
    "    logger.info(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the GRPO trainer...\n",
      "\n",
      "‚úÖ Added evaluation callback for progress tracking\n",
      "\n",
      "‚úÖ GRPO Trainer created successfully!\n",
      "\n",
      "Training Summary:\n",
      "  Model: DeepSeek Math 7B\n",
      "  Dataset: GSM8K math problems\n",
      "  Training examples: 5,978\n",
      "  Generations per prompt: 12\n",
      "  Effective batch size: 64\n",
      "  Total training steps: ~465\n",
      "\n",
      "The model will now learn to solve math problems better!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Create the GRPO Trainer\n",
    "# ============================================\n",
    "\n",
    "print(\"Creating the GRPO trainer...\\n\")\n",
    "\n",
    "# Initialize the GRPO trainer with all the components\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,  # The language model to train\n",
    "    reward_funcs=compute_rewards,  # Function to compute rewards\n",
    "    args=GRPO_config,  # Training configuration\n",
    "    train_dataset=train_dataset,  # Training data\n",
    "    eval_dataset=eval_dataset,  # Validation data\n",
    "    processing_class=tokenizer,  # Tokenizer\n",
    ")\n",
    "\n",
    "# Add the evaluation callback\n",
    "trainer.add_callback(eval_callback)\n",
    "print(\"‚úÖ Added evaluation callback for progress tracking\")\n",
    "\n",
    "print(\"\\n‚úÖ GRPO Trainer created successfully!\")\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"  Model: DeepSeek Math 7B\")\n",
    "print(f\"  Dataset: GSM8K math problems\")\n",
    "print(f\"  Training examples: {len(train_dataset):,}\")\n",
    "print(f\"  Generations per prompt: {config.num_generations}\")\n",
    "print(f\"  Effective batch size: {config.per_device_train_batch_size * config.gradient_accumulation_steps}\")\n",
    "print(f\"  Total training steps: ~{len(train_dataset) // (config.per_device_train_batch_size * config.gradient_accumulation_steps) * config.num_train_epochs}\")\n",
    "print(f\"\\nThe model will now learn to solve math problems better!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Train the Model with GRPO! (Ungraded Part) <a id=\"trainmodelwithgrpo\"></a>\n",
    "\n",
    "### What Happens During Training\n",
    "\n",
    "During each training step:\n",
    "1. **Select** a batch of math problems\n",
    "2. **Generate** 12 different solutions for each\n",
    "3. **Score** each solution with the reward model\n",
    "4. **Compare** solutions within each group\n",
    "5. **Update** the model to prefer better solutions\n",
    "\n",
    "### Training Time (With GPU)\n",
    "\n",
    "- To finish the full training, it will take dozens of hours. (Generally, it's not necessary to finish the full training schedule.)\n",
    "- With proper reward functions, you can notice that the Eval Acc is showing an upward trend within 1 or 2 hours of training\n",
    "- With proper reward functions, the Eval Acc could go above 40% within 10 to 15 hours of training.\n",
    "\n",
    "### What to Watch\n",
    "\n",
    "- **Loss**: Should decrease (model is learning)\n",
    "- **Accuracy**: Should increase (getting more problems right)\n",
    "- **Rewards**: Should increase (generating better solutions)\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "- Training will save checkpoints periodically\n",
    "- You can resume if training is interrupted\n",
    "- Evaluation results every 20 steps\n",
    "- Please analyze the logs under the grpo_logs folder to imporve the reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have successfully built the trainer. Next, you can use it to train GRPO. However, since the training process takes over 10 hours, it will not be conducted in this lab. You can run it on other GPU resources.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Start GRPO Training!\n",
    "# ============================================\n",
    "\n",
    "print(\"You have successfully built the trainer. Next, you can use it to train GRPO. However, since the training process takes over 10 hours, it will not be conducted in this lab. You can run it on other GPU resources.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Summary <a id=\"summary\"></a>\n",
    "\n",
    "### Congratulations!\n",
    "\n",
    "You've successfully completed the GRPO training lab! You've learned how to use reinforcement learning to improve a language model's ability to solve math problems.\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "1. **GRPO Fundamentals**\n",
    "   - How to generate multiple responses per prompt\n",
    "   - Why relative comparison works better than absolute rewards\n",
    "   - How group normalization stabilizes training\n",
    "\n",
    "2. **Practical Implementation**\n",
    "   - Setting up a reward model for math problems\n",
    "   - Configuring GRPO training parameters\n",
    "   - Monitoring training progress\n",
    "   - Evaluating model improvements\n",
    "\n",
    "3. **Key Insights**\n",
    "   - More generations per prompt = better comparison\n",
    "   - Partial credit rewards help learning\n",
    "   - Evaluation on held-out data is crucial\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To further improve your results:\n",
    "\n",
    "2. **Adjust Temperature**: Experiment with values between 0.5-1.0\n",
    "3. **Increase Generations**: Try 16 or 20 generations per prompt\n",
    "4. **Fine-tune Rewards**: Adjust the partial credit system\n",
    "5. **Try Other Datasets**: Apply GRPO to different tasks\n",
    "\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- **TRL Documentation**: [https://huggingface.co/docs/trl](https://huggingface.co/docs/trl)\n",
    "- **GRPO Paper**: [arXiv:2402.03300](https://arxiv.org/abs/2402.03300)\n",
    "- **GSM8K Dataset**: [OpenAI GitHub](https://github.com/openai/grade-school-math)\n",
    "- **DeepSeek Models**: [Hugging Face](https://huggingface.co/deepseek-ai)\n",
    "\n",
    "### Achievement Unlocked!\n",
    "\n",
    "You've successfully:\n",
    "- ‚úÖ Implemented GRPO from scratch\n",
    "- ‚úÖ Trained a 7B parameter model\n",
    "- ‚úÖ Improved math problem-solving accuracy\n",
    "- ‚úÖ Learned modern RL techniques for LLMs\n",
    "\n",
    "### Final Thoughts\n",
    "\n",
    "GRPO is just one of many post-training techniques. The same principles you've learned here apply to:\n",
    "- **PPO** (Proximal Policy Optimization)\n",
    "- **DPO** (Direct Preference Optimization)\n",
    "- **RLHF** (Reinforcement Learning from Human Feedback)\n",
    "\n",
    "The future of AI involves not just pre-training large models, but also fine-tuning them for specific tasks using techniques like GRPO. You're now equipped with the knowledge to be part of that future!\n",
    "\n",
    "### Thank You!\n",
    "\n",
    "Thank you for completing this lab. We hope you found it educational and enjoyable. Keep experimenting, keep learning, and keep pushing the boundaries of what's possible with AI!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
